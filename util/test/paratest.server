#!/usr/bin/env perl

# See print_help for usage instructions 
#
# Creating a file name PARAHALT in the root test directory halts the
# distribution of more work.
# (Do so on the host running paratest.server to avoid NFS delays if applicable.)
#
# Requirements:
#  - $CHPL_HOME environment variable is set
#  - paratest.server is run from $CHPL_HOME/test.
#  - Chapel compiler bin as $CHPL_HOME/bin/"chpl_bin_path"/chpl.
#  - Scripts start_test in $CHPL_HOME/util and paratest.client in the same 
#      directory as paratest.server. It will create a temporary directory 
#     .synch to synchronize the distribution of work to the client processes.
#  - Be able to run start_test remotely. This may include the following:
#    - Chapel built without node-specific local temporary directories.
#        Nodes must be able to execute start_test. For example, the
#        start_test script may invoke the compiler as ../bin/linux64/chpl.
#        If Chapel is built with CHPLDEVTMP defined to a machine-specific local
#        tmp directory (e.g., /ptmp), the script may not be able to execute
#        chpl on a different machine. A good check is to run start_test with 
#        a different machine to see if it can run successfully.
#    - To run jobs remotely over a secured network without having to
#      type a password each time, use ssh-agent and ssh-add. See
#      http://upc.lbl.gov/docs/user/sshagent.html for a short tutorial.
#


# for debugging
$debug = 0;                            # turn on debug output
$verbose = 0;                          # more verbose output

$logdir = "Logs";                      # dir under test to store logs
$synchdir = "$logdir/.synch";          # where to store temporary metadata
$rem_exe = "ssh -x";                   # disable X
$pwd = `pwd`; chomp $pwd;
$client_script = "$pwd/../util/test/paratest.client";
$venv_check = "$pwd/../util/test/activate_chpl_test_venv.py";
$summary_len = 2;
$sleep_time = 1;                       # polling time (sec) to distribute work
$futures_mode = 0;
$filedist = 0;
$dirs = "";
$node_para = 1;                        # the number of tasks to run on each node
$valgrind = 0;
$memleaks = 0;
$memleakslog = "/dev/null";
$memleaksflag = 0;
$show_all_errors = 0;
$junit_xml = 0;
$junit_xml_file = "";
$junit_remove_prefix = "";
# Once a node has timed out, we don't send it any more work, so this timeout 
# should be set higher than the time needed to process the largest directory.
# -1 means "never time out".
$timeout = -1;
if (exists $ENV{CHPL_PARATEST_TIMEOUT}) { $timeout = $ENV{CHPL_PARATEST_TIMEOUT}; } 

# updated if -valgrind options are given
$ENV{'CHPL_TEST_VGRND_COMP'} = "off";
$ENV{'CHPL_TEST_VGRND_EXE'} = "off";


$localhost = `uname -n`;
($localnode, $junk) = split (/\./, $localhost, 2);
chomp $localnode;


#
# clean up temporary files from previous parallel runs
#
system("rm -f $logdir/.-*.log $logdir/.-*log.summary");


my (@testdir_list, @node_list, $nodeCount, $starttime, $endtime);

# The callboard is used to ignore spawned processes that have timed out.
# Each entry contains the timestamp at which that node id was last fed.
my @callboard;

sub systemd {
    local ($cmd) = @_;
    print "$cmd\n" if $debug;
    system ($cmd);
}

sub trim {
  my $s = shift;
  $s =~ s/^\s+|\s+$//g;
  return $s;
}

# Collect individual logs into one final one.
sub collect_logs {
    local ($fin_log, @logs, $dead) = @_;
    local ($len, $successes, $failures, $futures);
    local ($grep_summ, $head_opts);

    print "collecting logs\n" if $debug;

    if ($platform eq "linux32") {
        $head_opts = "-q";                     # quiet mode
    }

    systemd ("echo \\[Parallel testing started at $starttime\\] > $fin_log");
    # Keep the individual logs upon failure.
    !$? or die "Could not set up final log '$fin_log': $?\n";

    print "Collecting logs to $fin_log\n";
    foreach $log (sort @logs) {
        if (-e $log) {
            print "Merging $log\n" if $verbose;
            open GLOG, $log or die "Cannot open log '$log'\n";
            $len = 0;
            while (<GLOG>) {
                last if (/^\[Test Summary/);
                $len++;
            }
            close GLOG;

            print "\nlen = $len\n" if $debug;
            systemd ("head $head_opts -n $len $log >> $fin_log");
            unlink $log if (-e $log);
            unlink "$log.summary" if (-e "$log.summary");
        }
    }

    local ($date) = `date +"%y%m%d.%H%M%S"`;
    chomp $date;

    # Generate the summary info
    systemd ("echo \\[Parallel testing started at $starttime\\] >> $fin_log");
    systemd ("echo \\[Parallel testing ended at $endtime\\] >> $fin_log");

    # Generate summary file
    $summ_log = "$fin_log.summary";
    unlink $summ_log if (-e $summ_log);
    systemd ("echo \\[Test Summary - $date\\] > $fin_log.summary");

    if ($dead > 0) {
        systemd("echo \\[Error: paratest failed to exit cleanly\\] >> $summ_log");
        for ($id = 0; $id < $nodeCount; $id++) {
            systemd("echo \\[Error: Worker $id on node $node_list[$id] timed out.\\] >> $summ_log")
                if $callboard[$id] > 0;
        }
    }

    systemd ("grep -a '^\\[Error' $fin_log >> $fin_log.summary");
    systemd ("grep -a '^Future' $fin_log >> $fin_log.summary");

    # Count stuff
    $successes = `grep -a "^\\[Success matching" $fin_log | wc -l`;
    $successes =~ s/\s//g;
    ($successes, $junk) = split (/\s+/, $successes, 2);
    $failures = `grep -a "^\\[Error" $fin_log.summary | wc -l`;
    $failures =~ s/\s//g;
    ($failures, $junk) = split (/\s+/, $failures, 2);
    $futures = `grep -a "^Future" $fin_log.summary | wc -l`;
    $futures =~ s/\s//g;
    ($futures, $junk) = split (/\s+/, $futures, 2);
    systemd ("echo \\[Summary: \\#Successes = $successes \\| \\#Failures = $failures \\| \\#Futures = $futures\\] >> $fin_log.summary");
    systemd ("echo \\[END\\] >> $fin_log.summary");

    systemd ("cat $fin_log.summary >> $fin_log");

    print "\n[Summary: #Successes = $successes | #Failures = $failures | #Futures = $futures]\n";

}


# Return a list of IDs of nodes ready to work
sub free_workers {
    local (@readyv, @readyids, $node, $id);
    print "checking for available workers\n" if $debug;

    opendir WORKDIR, "$synchdir";
    @readyv = readdir WORKDIR;
    closedir WORKDIR;

    foreach $ready (@readyv) {
        next if ($ready =~ /^\./);
        my ($node, $id, $failed) = split (/\./, $ready);
        if ($failed) {
            print "\n failure on $node, no more testing there\n";
            unlink "$synchdir/$ready";
        } else {
            push @readyids, $id;
            # Wait on the processes represented by each of the files being iterated
            # over. The existence of each file implies a previously forked process
            # has finished and is currently a zombie. The only exception to this
            # is the first time this subroutine is called. The first time, all of
            # these files were created by this driver script, not children. This
            # is not a problem though since wait is non-blocking.
            wait;
        }
    }

    print (@readyids ? "," : ".");
    return @readyids;
}


# While there is still work to do, continually feed the  nodes
# bits of work. If the file "PARAHALT" exists in the test directory,
# distribution of work stops.  This is one hack to stop the testing.
# Of course, you'll have to wait on the client processes or kill them
# manually.
sub feed_nodes {
    my $chplenv = $_[0];
    local (@readyidv, $logfile, @logs, $testdir, $node, $rem_cmd);
    $nodeCount = $#node_list + 1;

    $| = 1;    # autoflush stdout

    print "about to start distributing work\n" if $debug;
   
    $activate_venv_output = `$venv_check`;
    if ($? != 0) {
      print $activate_venv_output;
      systemd ("echo '$activate_venv_output' >> $fin_logfile");
    } else {
      @testdir_list = sort @testdir_list;
      print $nodeCount; print " worker(s) (@node_list)\n";
      print "timeout = $timeout\n" if $debug > 0;
      my $startCount = $#testdir_list + 1;
      print $startCount; print " test(s) (@testdir_list)\n";

      my $startSecs = time();
      while (($#testdir_list >= 0) &&       # while still have work to do
             ($nodeCount > 0)      &&       # not all nodes have failed
             !(-e "PARAHALT")) {
          @readyidv = free_workers ();      # get IDs of nodes that are ready

          print @readyidv if $debug;
          print "\n" if ($#readyidv >= 0);
          foreach $readyid (@readyidv) {    # for ready nodes
              next if ($#testdir_list < 0);

              $testdir = $testdir_list[0];
              $node = $node_list[$readyid]; # machine name to rem exec to
              $synchfile = "$synchdir/$node.$readyid";
              $callboard[$readyid] = time();

              # remove synch file before forking work to worker
              unless (-e $synchfile) {
                  printf ("Error: synch file '$synchfile' missing\n");
                  exit (7);
              }
              unlink $synchfile;

              my $elapsedSecs = time() - $startSecs;
              my $testsLeft = $#testdir_list + $nodeCount;
              my $testsDone = $startCount - $testsLeft;
              my $estSecsLeft = 1;
              if ($testsDone > 0) {
                  $estSecsLeft = int($testsLeft * $elapsedSecs / $testsDone);
              }
              my ($endSec, $endMin, $endHour) = localtime(time() + $estSecsLeft);

              print "$node <- $testdir ($#testdir_list left, ";
              printf ("end ~%02d:%02d:%02d)\n", $endHour, $endMin, $endSec);
              $testdirname = $testdir;
              $testdirname =~ s/\//-/g;
              $logfile = "$logdir/$testdirname.$node.log";
              # fork work
              unless ($pid = fork) {        # child
                  if ($node eq $localnode) {
                      $rem_exec_cmd = "";
                      $chplenv = "\"$chplenv\"";
                      $compopts = "\"$compopts\"";
                      $execopts = "\"$execopts\"";
                  } else {
                      $rem_exec_cmd = "$rem_exe $node";
                      $chplenv = "\\\"$chplenv\\\"";
                      $compopts = "\\\"$compopts\\\"";
                      $execopts = "\\\"$execopts\\\"";
                  }
                  # If the command line gets too long, consider using xargs
                  $rem_cmd = "$rem_exec_cmd $client_script $readyid $pwd $testdir $chplenv $futures_mode $valgrind $memleaksflag $compopts $execopts";
                  if ($verbose) {
                      systemd ($rem_cmd);
                  } else {
                      systemd ("$rem_cmd 2>&1 | grep -i 'Error in paratest.client:'");
                  }
                  $pe_command = $show_all_errors
                    # print Error lines; terminate when we reach Test Summary
                    ? '/^\\[Error/p; /^\\[Test Summary/q'
                    # once an Error line is seen, print it and terminate
                    : '/^\\[Error/{p;q;}';
                  $partial_errors = `sed -n '$pe_command' $logfile`;
                  if ($partial_errors) {
                      print "\n:( $partial_errors";
                  } else {
                      print ":)";
                  }
                  exit (0);
              }

              push @logs, $logfile;
              shift @testdir_list;
          }

          # wait before checking for free workers
          sleep $sleep_time;                          
      }

      # wait for everyone to finish;
      my ($done, $dead) = (0, 0);
      while (! (-e "PARAHALT")) {
          # For workers that are ready, set their last-fed time to zero.
          @readyidv = free_workers ();
          foreach $readyid (@readyidv) {
              $callboard[$readyid] = 0;

              # This worker's termination has been noted.
              $node = $node_list[$readyid]; # machine name to rem exec to
              $synchfile = "$synchdir/$node.$readyid";
              unless (-e $synchfile) {
                  printf ("Error: synch file '$synchfile' missing\n");
                  exit (7);
              }
              unlink $synchfile;
          }

          # Now check that all nodes have timed out.
          ($done, $dead) = (0, 0);
          for ($id = 0; $id < $nodeCount; $id++) {
              if ($callboard[$id] == 0) { $done++; next; }
              $dead++ if ($timeout > 0) && ((time() - $callboard[$id]) > $timeout);
          }
          last if ($done + $dead) >= $nodeCount;

          sleep $sleep_time;
      }

      # Note that hung processes on remote nodes must be killed manually
      # if we exit this loop due to one or more workers being declared dead.
      if ($dead > 0) {
          print "\n[Error: paratest failed to exit cleanly]\n";
          for ($id = 0; $id < $nodeCount; $id++) {
              print "[Error: Worker $id on node $node_list[$id] timed out.]\n"
                  if $callboard[$id] > 0;
          }
      }

      if (-e "PARAHALT" && ($#testdir_list > 0)) {
          print "\nExiting early due to PARAHALT file\n";
      }

      if ($#testdir_list >= 0) {
         print $#testdir_list+1; 
         print " directory(s) left untested: @testdir_list\n";
      }

      $endtime = `date`; chomp $endtime;
      print "\n";
      if ($memleaksflag == 2) {
          print "Collecting memleaks logs to $memleakslog\n";
          systemd("cat $logdir/tmp.*.memleaks > $memleakslog");
          # Keep the individual logs upon failure.
          !$? or die "Could not collect memleaks logs: $?\n";
          systemd("rm -f $logdir/tmp.*.memleaks");
      }
    }
    collect_logs ($fin_logfile, @logs, $dead);
}


sub generate_junit_xml {
    $junit_args = "--start-test-log=$fin_logfile";
    if (!($junit_xml_file eq "")) {
        $junit_args = "$junit_args --junit-xml=$junit_xml_file"
    }
    if (!($junit_remove_prefix eq "")) {
        $junit_args = "$junit_args --remove-prefix=$junit_remove_prefix"
    }
    if ($junit_xml == 1) {
        print "[Generating jUnit XML report]\n";
        systemd("$pwd/../util/test/convert_start_test_log_to_junit_xml.py $junit_args");
    }
}


# Signal that all nodes are free to do some work by writing their
# synchronization files for them initially.
sub nodes_free {
    local ($id, $node, $fname, $dirv);

    print "making all nodes available to work\n" if $debug;
    # clean synch dir
    opendir WORKDIR, "$synchdir";
    @dirv = readdir WORKDIR;
    closedir WORKDIR;
    foreach $file (@dirv) {
        unlink "$synchdir/$file";
    }

    systemd ("rm -f $synchdir/*");
    # signal that all nodes are free
    my $nodeCount = $#node_list + 1;
    for ($id=0; $id<$nodeCount; $id++) {
        $fname = "$node_list[$id].$id";
        systemd ("echo feed me > $synchdir/$fname");
    }
}


# Return true if a *.chpl, *.test.c or *.ml-test.c exists. Otherwise false.
sub chpl_files {
    local (@fnames) = @_;
    local ($found);
    $found = 0;
    foreach $fname (@fnames) {
        if ($fname eq "NOTEST") {
            return 0;
        }
        if ($fname =~ /\.chpl$/     ||
            $fname =~ /\.test\.c$/  ||
            $fname =~ /\.ml-test\.c$/ ||
	    $fname eq "sub_test"
	) {
	    # still a NOTEST would override
            $found = 1;
        }
    }
    return $found;
}


# Gather all the subdirectories into a flat list and return it.
sub find_subdirs {
    local ($targetdir, $level) = @_;
    local ($filen, @cdir, @founddirs, $i);

    print "looking in '$targetdir'\n" if $debug;
    opendir CURRDIR, $targetdir or die "Cannot open directory '$targetdir'\n";
    @cdir = grep !/^\./, readdir CURRDIR;             # curr dir list of files
    closedir CURRDIR;

    if (chpl_files (@cdir)) {                         # if *.chpl files in dir
	push @founddirs, $targetdir;
	print "$targetdir\n" if $debug;
    }
    foreach $filen (@cdir) {
        if (-d "$targetdir/$filen") {                 # if dir
            # .skipif does double duty here
            # When attached to a directory name, skips it and all descendents if the condition is true.
            my $skipfilen = "$targetdir/$filen.skipif";
            if (-e "$skipfilen") {
                my $skip;
                print "$ENV{CHPL_HOME}/util/test/testEnv $skipfilen = " . `$ENV{CHPL_HOME}/util/test/testEnv $skipfilen` if $debug;
                $skip = `$ENV{CHPL_HOME}/util/test/testEnv $skipfilen`;
                next if $skip > '0' or $skip =~ m/true/i;
            }
            # .notest does double duty here as well
            # When attached to a directory name, skips it and all descendents
            my $notestfilen = "$targetdir/$filen.notest";
            if (-e "$notestfilen") {
                next;
            }
            if ($debug) {for ($i=0; $i<$level; $i++)  {print "    ";}}
                if (! -l "$targetdir/$filen") {
                    push @founddirs, find_subdirs ("$targetdir/$filen", $level+1);
                }
        }
    }
    return @founddirs;
}


# Gather the list of files to test and return it.
sub find_files {
    local ($targetdir, $level, $no_futures, $recursive) = @_;
    local ($filen, @cdir, @foundfiles);

    print "looking in '$targetdir'\n" if $debug;
    opendir CURRDIR, $targetdir or die "Cannot open directory '$targetdir'\n";
    @cdir = grep !/^\./, readdir CURRDIR;             # curr dir list of files
    closedir CURRDIR;

    foreach $filen (@cdir) {
	$filepath = "$targetdir/$filen";
	unless (-e "$targetdir/NOTEST") {             # do not ignore this dir?
	    if ($filepath =~ /\.chpl$/) {
		$futuref = $filepath;
		$futuref =~ s/\.chpl$/.future/;
		next if ((-e $futuref) && $no_futures);
		push @foundfiles, $filepath;
	    } elsif ($filepath =~ /\.test\.c$/) {
		$futuref = $filepath;
		$futuref =~ s/\.test\.c$/.future/;
		next if ((-e $futuref) && $no_futures);
		push @foundfiles, $filepath;
	    } elsif ($filepath =~ /\.ml-test\.c$/) {
		$futuref = $filepath;
		$futuref =~ s/\.ml-test\.c$/.future/;
		next if ((-e $futuref) && $no_futures);
		push @foundfiles, $filepath;
	    }
	}

	if ((-d $filepath) && $recursive) {           # if dir and recursive
	    if ($debug) {for ($i=0; $i<$level; $i++)  {print "    ";}}
	    push @foundfiles, find_files ($filepath, $level+1, $no_futures, $recursive);
        }
    }
    return @foundfiles;
}


sub print_help {
    print "Usage: paratest.server [-compopts s] [-dirfile d] [-dirs d] [-env s] [-execopts s] [-filedist] [-futures] [-futures-only] [-logfile l] [-memleaks] [-memleakslog f] [-multilocale-only] [-nodefile n] [-nodepara m] [-valgrind[exe]] [-help|-h] [-timeout t]\n";
    print "    -compopts s: s is a string that is passed with -compopts to start_test.\n";
    print "    -dirfile  d: d is a file listing directories to test. Default is the current diretory.\n";
    print "    -dirs     d: d is a space separated list of directories to recursively search for directories to test\n";
    print "    -env s     : s is a string of space separated env vars forwarded to each client\n";
    print "    -execopts s: s is a string that is passed with -execopts to start_test.\n";
    print "    -filedist  : distribute work at the granularity of files (directory granurality is the default).\n";
    print "    -futures   : include .future tests (default is none).\n";
    print "    -futures-only : only run .future tests, not regular ones.\n";
    print "    -logfile  l: l is the output log file. Default is \"user\".\"platform\".log. in the Logs subdirectory.\n";
    print "    -memleaks  : pass -memleaks to start_test. \n";
    print "    -memleakslog f: pass -memleakslog to start_test; aggregate all output.\n";
    print "    -multilocale-only: pass -multilocale-only to start_test if comm!=none\n";
    print "    -nodefile n: n is a file listing nodes to run on. Default is current node.\n";
    print "    -nodepara m: Run m paratest.client tasks on each node.\n";
    print "    -valgrind[exe]  : pass -valgrind or -valgrindexe to start_test.\n";
    print "    -timeout  t: t is the max time to wait after last directory is served.\n";
    print "    -junit-xml : Create jUnit test report.\n";
    print "    -junit-xml-file f: Put jUnit test report at location 'f' (implies -junit-xml).\n";
    print "    -junit-remove-prefix p: Remove prefix 'p' from all tests in junit report.\n";
}


sub main {
    local ($id, $synchfile);
    
    $user = `whoami`; chomp $user;
    $platform = `../util/chplenv/chpl_platform.py`; chomp $platform;
    $fin_logfile = "$logdir/$user.$platform.log";      # final log file name
    # $fin_logfile = "$logdir/$user.$platform.log";
    unlink $fin_logfile if (-e $fin_logfile);          # remove final log file

    $starttime = `date`; chomp $starttime;

    while ($#ARGV >= 0) {
        $_ = $ARGV[0];
        if (/^-compopts$/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $compopts = "$compopts $ARGV[0]";
            } else {
                print "missing -compopts arg\n";
                exit (8);
            }
        } elsif (/^-execopts$/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $execopts = "$execopts $ARGV[0]";
            } else {
                print "missing -execopts arg\n";
                exit (8);
            }
        } elsif (/^-env$/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $extra_env = "$extra_env $ARGV[0]";
            } else {
                print "missing -env arg\n";
                exit (8);
            }
        } elsif (/^-filedist$/) {
            $filedist = 1;
        } elsif (/^-dirfile$/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $dirfile = $ARGV[0];
            } else {
                print "missing -dirfile arg\n";
                exit (8);
            }
        } elsif (/^-dirs$/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $dirs = trim("$dirs $ARGV[0]");
            } else {
                print "missing -dirs arg\n";
                exit (8);
            }
         # "undocumented" option, only meant to be used by other scripts
        } elsif (/^-futures-mode$/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $futures_mode = $ARGV[0];
            } else {
                print "missing -futures-mode arg\n";
                exit (8);
            }
        } elsif (/^-futures-only$/) {
            $futures_mode = 2;
        } elsif (/^-futures$/) {
            $futures_mode = 1;
        } elsif (/^-logfile$/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $fin_logfile = $ARGV[0];
            } else {
                print "missing -logfile arg\n";
                exit (8);
            }
        } elsif (/^-nodefile$/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $nodefile = $ARGV[0];
            } else {
                print "missing -nodefile arg\n";
                exit (8);
            }
        } elsif (/^-nodepara$/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $node_para = $ARGV[0];
            } else {
                print "missing -nodepara arg\n";
                exit(8);
            }
        } elsif (/^-timeout$/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $timeout = $ARGV[0];
            } else {
                print "missing -timeout arg\n";
                exit(8);
            }
        } elsif (/^-memleaks$/) {
            $memleaksflag = 1;
        } elsif (/^-memleakslog$/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $memleakslog = $ARGV[0];
                $memleaksflag = 2;
            } else {
                print "missing -memleakslog arg\n";
                exit (8);
            }
        } elsif (/^-multilocale-only$/) {
            $comm = `$ENV{CHPL_HOME}/util/chplenv/chpl_comm.py`; chomp $comm;
            if ($comm ne "none") {
              $ENV{'CHPL_TEST_MULTILOCALE_ONLY'} = "true";
              $extra_env = "$extra_env CHPL_TEST_MULTILOCALE_ONLY=true";
            }
        } elsif (/^-valgrind$/) {
            $valgrind = 1;
            $ENV{'CHPL_TEST_VGRND_COMP'} = "on";
            $ENV{'CHPL_TEST_VGRND_EXE'} = "on";
        } elsif (/^-valgrindexe$/) {
            $valgrind = 2;
            $ENV{'CHPL_TEST_VGRND_EXE'} = "on";
        } elsif (/^-show-all-errors$/) {
            $show_all_errors = 1;
        } elsif (/^-junit-xml$/) {
            $junit_xml = 1;
        } elsif (/^-junit-xml-file$/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $junit_xml_file = $ARGV[0];
                $junit_xml = 1;
            } else {
                print "missing -junit-xml-file arg\n";
                exit (8);
            }
        } elsif (/^-junit-remove-prefix$/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $junit_remove_prefix = $ARGV[0];
            } else {
                print "missing -junit-remove-prefix arg\n";
                exit (8);
            }
        } elsif (/^-help$|^-h$/) {
            print_help;
            exit (9);
        } else {
            print "unknown arg $_\n";
            exit (9);
        }
        shift @ARGV;
    }

    die "Error: CHPL_HOME must be set\n" unless defined $ENV{CHPL_HOME};

    # Check for a compatible valgrind configuration
    if ($valgrind > 0) {
      $tasks = `$ENV{CHPL_HOME}/util/chplenv/chpl_tasks.py`; chomp $tasks;
      $mem = `$ENV{CHPL_HOME}/util/chplenv/chpl_mem.py --target`; chomp $mem;
      $regexp = `$ENV{CHPL_HOME}/util/chplenv/chpl_regexp.py`; chomp $regexp;
      $re2_valgrind = `$ENV{CHPL_HOME}/util/test/re2_supports_valgrind.py`; chomp $re2_valgrind;

      if ($tasks ne "fifo") {
        die "Error: valgrind requires tasks=fifo - try quickstart";
      }
      if ($mem ne "cstdlib") {
        die "Error: valgrind requires mem=cstdlib - try quickstart";
      }
      if ($regexp eq "re2" && $re2_valgrind eq "False") {
        die "Error: valgrind requires regexp=none or re2 built with CHPL_RE2_VALGRIND_SUPPORT";
      }
    }

# Set some more environment variables so they can be tested by skipif
    $ENV{'COMPOPTS'} = $compopts;
    $ENV{'EXECOPTS'} = $execopts;

    if (defined $nodefile) {
        open nodefile or die "Cannot open node file '$nodefile'\n";
        while (<nodefile>) {
            next if /^$|^\#/;
            chomp;
            if (trim($_) eq "localhost") {
              $_ = $localnode;
            }
            $count = $node_para;
            while ($count--) {
                push @node_list, $_;
            }
        }
    } else { # else, just current node
        if( (defined $ENV{'SLURM_JOB_NODELIST'}) &&
            ($platform !~ /^cray-x/) ) {
            my $SLURM_NODELIST = $ENV{'SLURM_JOB_NODELIST'};
            my $SLURM_NODES = `scontrol show hostnames $SLURM_NODELIST`;
            my @SLURM_NODES = split(' ', $SLURM_NODES);
            for my $node (@SLURM_NODES) {
                print "using node $node from SLURM\n";
                $count = $node_para;
                while ($count--) {
                    push @node_list, $node;
                }
            }
        } else {
            while ($node_para--) {
                push @node_list, $localnode;
            }
        }
    }

    if (defined $dirfile) {
        open dirfile or die "Cannot open directory file '$dirfile'\n";
        while (<dirfile>) {
            next if /^$|^\#/;
            chomp;
            if ($filedist) {
                push @testdir_list, find_files( $_, 0, !$futures_mode, 0);
            } else {
                push @testdir_list, $_;
            }
        }
    } elsif ($dirs ne "") {
        print "[Collecting test directories in '$dirs']\n";
        my @dirs_list = split ' ', $dirs;
        for my $dir (@dirs_list) {
            if (! -d $dir) {
                print "Error: '$dir' is not a directory\n";
                exit (2);
            }
            if ($filedist) {
                push @testdir_list, find_files ("$dir", 0, !$futures_mode, 1);
            } else {
                push @testdir_list, find_subdirs ("$dir", 0);
            }
        }
    } else { # else, current working dir
        use Cwd;
        my $cwd = &Cwd::cwd();
        print "[Generating tests from the Chapel Spec in $ENV{CHPL_HOME}/spec]\n";
        chdir $ENV{CHPL_HOME} or die "Can't cd to $ENV{CHPL_HOME}: $!\n";
        my $autogen=`make spectests`;
        die "Error generating Spec tests in $ENV{CHPL_HOME}/spec\n" unless $? == 0;
        chdir $cwd;
        if ($filedist) {
            print "[Collecting test files in $cwd]\n";
            @testdir_list = find_files (".", 0, !$futures_mode, 1);
        } else {
            print "[Collecting test directories in $cwd]\n";
            @testdir_list = find_subdirs (".", 0);
        }
    }

    unless (-e $logdir) {
        print "Error: log directory $logdir does not exist\n";
        exit (2);
    }
    unless (-e "$synchdir") { 
        systemd ("mkdir $synchdir"); 
    }

    # Package up the environment
    my $tmpchplenv = `$ENV{CHPL_HOME}/util/printchplenv --simple --all --overrides`;
    my $chplenv = "";
    my @lines = split(/\n/, $tmpchplenv);
    for my $line (@lines) {
        $line =~ s/export\s+//;
        my ($key,$value) = split(/=/, $line, 2);
        $chplenv .= "$key=$value ";
    }
    $chplenv="$chplenv$extra_env";

    nodes_free ();         # signal that all nodes free
    feed_nodes ($chplenv); # parallel testing

    # cleanup - remove synch files and synch dir
    for ($id=0; $id<=$#node_list; $id++) {
        $synchfile = "$synchdir/$node_list[$id].$id";
        unlink $synchfile if (-e $synchfile);
    }
    rmdir $synchdir;

    # generate jUnit report
    generate_junit_xml();
}


main ();

