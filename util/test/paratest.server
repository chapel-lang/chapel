#!/usr/bin/env perl

# See print_help for usage instructions 
#
# Creating a file name PARAHALT in the root test directory halts the
# distribution of more work.
# (Do so on the host running paratest.server to avoid NFS delays if applicable.)
#
# Requirements:
#  - $CHPL_HOME environment variable is set
#  - paratest.server is run from $CHPL_HOME/test.
#  - Chapel compiler bin as $CHPL_HOME/bin/"platform"/chpl.
#  - Scripts start_test in $CHPL_HOME/util and paratest.client and
#      filterSuppressions in the same directory as paratest.server. It
#      will create a temporary directory (.synch) to synchronize the
#      distribution of work to the client processes.
#  - Be able to run start_test remotely. This may include the following:
#    - Chapel built without node-specific local temporary directories.
#        Nodes must be able to execute start_test. For example, the
#        start_test script may invoke the compiler as ../bin/linux64/chpl.
#        If Chapel is built with CHPLDEVTMP defined to a machine-specific local
#        tmp directory (e.g., /ptmp), the script may not be able to execute
#        chpl on a different machine. A good check is to run start_test with 
#        a different machine to see if it can run successfully.
#    - To run jobs remotely over a secured network without having to
#      type a password each time, use ssh-agent and ssh-add. See
#      http://upc.lbl.gov/docs/user/sshagent.html for a short tutorial.
#


# for debugging
$debug = 0;                            # turn on debug output
$verbose = 0;                          # more verbose output

$dirs_to_ignore = "Bin|Logs|Samples|Share|OUTPUT|RCS";

$logdir = "Logs";                      # dir under test to store logs
$synchdir = "$logdir/.synch";          # where to store temporary metadata
$rem_exe = "ssh -x";                   # disable X
$pwd = `pwd`; chomp $pwd;
$client_script = "$pwd/../util/test/paratest.client";
$summary_len = 2;
$sleep_time = 1;                       # polling time (sec) to distribute work
$futures_mode = 0;
$filedist = 0;
$node_para = 1;				                 # the number of tasks to run on each node
$valgrind = 0;
$memleaks = "/dev/null";
$memleaksflag = 0;
$show_all_errors = 0;
$suppressions = "";
$junit_xml = 0;
$junit_xml_file = "";
$junit_remove_prefix = "";
# Once a node has timed out, we don't send it any more work, so this timeout 
# should be set higher than the time needed to process the largest directory.
# -1 means "never time out".
$timeout = -1;
if (exists $ENV{CHPL_PARATEST_TIMEOUT}) { $timeout = $ENV{CHPL_PARATEST_TIMEOUT}; } 

$localhost = `uname -n`;
($localnode, $junk) = split (/\./, $localhost, 2);
chomp $localnode;


#
# clean up temporary files from previous parallel runs
#
system("rm -f $logdir/.-*.log $logdir/.-*log.summary");


my (@testdir_list, @node_list, $nodeCount, $starttime, $endtime);

# The callboard is used to ignore spawned processes that have timed out.
# Each entry contains the timestamp at which that node id was last fed.
my @callboard;

sub systemd {
    local ($cmd) = @_;
    print "$cmd\n" if $debug;
    system ($cmd);
}


# Collect individual logs into one final one.
sub collect_logs {
    local ($fin_log, @logs, $dead) = @_;
    local ($len, $successes, $failures, $futures);
    local ($grep_summ, $head_opts);

    print "collecting logs\n" if $debug;

    if ($platform eq "linux32") {
        $head_opts = "-q";                     # quiet mode
    }

    systemd ("echo \\[Parallel testing started at $starttime\\] > $fin_log");
    print "Collecting logs to $fin_log\n" if $verbose;
    foreach $log (@logs) {
        if (-e $log) {
            print "Merging $log\n" if $verbose;
            open GLOG, $log or die "Cannot open log '$log'\n";
            $len = 0;
            while (<GLOG>) {
                last if (/^\[Test Summary/);
                $len++;
            }
            close GLOG;

            print "\nlen = $len\n" if $debug;
            systemd ("head $head_opts -n $len $log >> $fin_log");
            unlink $log if (-e $log);
            unlink "$log.summary" if (-e "$log.summary");
        }
    }

    local ($date) = `date +"%y%m%d.%H%M%S"`;
    chomp $date;

    # Generate the summary info
    systemd ("echo \\[Parallel testing started at $starttime\\] >> $fin_log");
    systemd ("echo \\[Parallel testing ended at $endtime\\] >> $fin_log");

    # Generate summary file
    $summ_log = "$fin_log.summary";
    unlink $summ_log if (-e $summ_log);
    systemd ("echo \\[Test Summary - $date\\] > $fin_log.summary");

    if ($dead > 0) {
        systemd("echo \\[Error: paratest failed to exit cleanly\\] >> $summ_log");
        for ($id = 0; $id < $nodeCount; $id++) {
            systemd("echo \\[Error: Worker $id on node $node_list[$id] timed out.\\] >> $summ_log")
                if $callboard[$id] > 0;
        }
    }

    systemd ("grep -a '^\\[Error' $fin_log >> $fin_log.summary");
    if (!($suppressions eq "")) {
        systemd ("$pwd/../util/test/filterSuppressions $suppressions $fin_log.summary");
    }
    systemd ("grep -a '^Future' $fin_log >> $fin_log.summary");

    # Count stuff
    $successes = `grep -a "^\\[Success matching" $fin_log | wc -l`;
    $successes =~ s/\s//g;
    ($successes, $junk) = split (/\s+/, $successes, 2);
    $failures = `grep -a "^\\[Error" $fin_log.summary | wc -l`;
    $failures =~ s/\s//g;
    ($failures, $junk) = split (/\s+/, $failures, 2);
    $futures = `grep -a "^Future" $fin_log.summary | wc -l`;
    $futures =~ s/\s//g;
    ($futures, $junk) = split (/\s+/, $futures, 2);
    systemd ("echo \\[Summary: \\#Successes = $successes \\| \\#Failures = $failures \\| \\#Futures = $futures\\] >> $fin_log.summary");
    systemd ("echo \\[END\\] >> $fin_log.summary");

    systemd ("cat $fin_log.summary >> $fin_log");

    print "\n[Summary: #Successes = $successes | #Failures = $failures | #Futures = $futures]\n";

}


# Return a list of IDs of nodes ready to work
sub free_workers {
    local (@readyv, @readyids, $node, $id);
    print "checking for available workers\n" if $debug;

    opendir WORKDIR, "$synchdir";
    @readyv = readdir WORKDIR;
    closedir WORKDIR;

    foreach $ready (@readyv) {
        next if ($ready =~ /^\./);
        my ($node, $id, $failed) = split (/\./, $ready);
        if ($failed) {
            print "\n failure on $node, no more testing there\n";
            unlink "$synchdir/$ready";
        } else {
            push @readyids, $id;
            # Wait on the processes represented by each of the files being iterated
            # over. The existence of each file implies a previously forked process
            # has finished and is currently a zombie. The only exception to this
            # is the first time this subroutine is called. The first time, all of
            # these files were created by this driver script, not children. This
            # is not a problem though since wait is non-blocking.
            wait;
        }
    }

    print (@readyids ? "," : ".");
    return @readyids;
}


# While there is still work to do, continually feed the  nodes
# bits of work. If the file "PARAHALT" exists in the test directory,
# distribution of work stops.  This is one hack to stop the testing.
# Of course, you'll have to wait on the client processes or kill them
# manually.
sub feed_nodes {
    my $chplenv = $_[0];
    local (@readyidv, $logfile, @logs, $testdir, $node, $rem_cmd);
    $nodeCount = $#node_list + 1;

    $| = 1;    # autoflush stdout

    print "about to start distributing work\n" if $debug;

    @testdir_list = sort @testdir_list;
    print $nodeCount; print " worker(s) (@node_list)\n";
    print "timeout = $timeout\n" if $debug > 0;
    my $startCount = $#testdir_list + 1;
    print $startCount; print " test(s) (@testdir_list)\n";

    my $startSecs = time();
    while (($#testdir_list >= 0) &&       # while still have work to do
           ($nodeCount > 0)      &&       # not all nodes have failed
           !(-e "PARAHALT")) {
        @readyidv = free_workers ();      # get IDs of nodes that are ready

        print @readyidv if $debug;
        print "\n" if ($#readyidv >= 0);
        foreach $readyid (@readyidv) {    # for ready nodes
            next if ($#testdir_list < 0);

            $testdir = $testdir_list[0];
            $node = $node_list[$readyid]; # machine name to rem exec to
            $synchfile = "$synchdir/$node.$readyid";
            $callboard[$readyid] = time();

            # remove synch file before forking work to worker
            unless (-e $synchfile) {
                printf ("Error: synch file '$synchfile' missing\n");
                exit (7);
            }
            unlink $synchfile;

            my $elapsedSecs = time() - $startSecs;
            my $testsLeft = $#testdir_list + $nodeCount;
            my $testsDone = $startCount - $testsLeft;
            my $estSecsLeft = 1;
            if ($testsDone > 0) {
                $estSecsLeft = int($testsLeft * $elapsedSecs / $testsDone);
            }
            my ($endSec, $endMin, $endHour) = localtime(time() + $estSecsLeft);

            print "$node <- $testdir ($#testdir_list left, ";
            printf ("end ~%02d:%02d:%02d)\n", $endHour, $endMin, $endSec);
            $testdirname = $testdir;
            $testdirname =~ s/\//-/g;
            $logfile = "$logdir/$testdirname.$node.log";
            # fork work
            unless ($pid = fork) {        # child
                if ($node eq $localnode) {
                    $rem_exec_cmd = "";
                    $chplenv = "\"$chplenv\"";
                    $compopts = "\"$compopts\"";
                    $execopts = "\"$execopts\"";
                } else {
                    $rem_exec_cmd = "$rem_exe $node";
                    $chplenv = "\\\"$chplenv\\\"";
                    $compopts = "\\\"$compopts\\\"";
                    $execopts = "\\\"$execopts\\\"";
                }
                # If the command line gets too long, consider using xargs
                $rem_cmd = "$rem_exec_cmd $client_script $readyid $pwd $testdir $chplenv $futures_mode $valgrind $memleaksflag $compopts $execopts";
                if ($verbose) {
                    systemd ($rem_cmd);
                } else {
                    systemd ("$rem_cmd 2>&1 | grep -i 'Error in paratest.client:'");
                }
                $pe_command = $show_all_errors
                  # print Error lines; terminate when we reach Test Summary
                  ? '/^\\[Error/p; /^\\[Test Summary/q'
                  # once an Error line is seen, print it and terminate
                  : '/^\\[Error/{p;q}';
                $partial_errors = `sed -n '$pe_command' $logfile`;
                if ($partial_errors) {
                    print "\n:( $partial_errors";
                } else {
                    print ":)";
                }
                exit (0);
            }

            push @logs, $logfile;
            shift @testdir_list;
        }

        # wait before checking for free workers
        sleep $sleep_time;                          
    }

    # wait for everyone to finish;
    my ($done, $dead) = (0, 0);
    while (! (-e "PARAHALT")) {
        # For workers that are ready, set their last-fed time to zero.
        @readyidv = free_workers ();
        foreach $readyid (@readyidv) {
            $callboard[$readyid] = 0;

            # This worker's termination has been noted.
            $node = $node_list[$readyid]; # machine name to rem exec to
            $synchfile = "$synchdir/$node.$readyid";
            unless (-e $synchfile) {
                printf ("Error: synch file '$synchfile' missing\n");
                exit (7);
            }
            unlink $synchfile;
        }

        # Now check that all nodes have timed out.
        ($done, $dead) = (0, 0);
        for ($id = 0; $id < $nodeCount; $id++) {
            if ($callboard[$id] == 0) { $done++; next; }
            $dead++ if ($timeout > 0) && ((time() - $callboard[$id]) > $timeout);
        }
        last if ($done + $dead) >= $nodeCount;

        sleep $sleep_time;
    }

    # Note that hung processes on remote nodes must be killed manually
    # if we exit this loop due to one or more workers being declared dead.
    if ($dead > 0) {
        print "\n[Error: paratest failed to exit cleanly]\n";
        for ($id = 0; $id < $nodeCount; $id++) {
            print "[Error: Worker $id on node $node_list[$id] timed out.]\n"
                if $callboard[$id] > 0;
        }
    }

    if (-e "PARAHALT" && ($#testdir_list > 0)) {
        print "\nExiting early due to PARAHALT file\n";
    }

    if ($#testdir_list >= 0) {
       print $#testdir_list+1; 
       print " directory(s) left untested: @testdir_list\n";
    }

    $endtime = `date`; chomp $endtime;
    if ($memleaksflag) {
        systemd("cat $logdir/tmp.*.memleaks > $memleaks");
        systemd("rm -f $logdir/tmp.*.memleaks");
    }
    collect_logs ($fin_logfile, @logs, $dead);
}


sub generate_junit_xml {
    $junit_args = "--start-test-log=$fin_logfile";
    if (!($junit_xml_file eq "")) {
        $junit_args = "$junit_args --junit-xml=$junit_xml_file"
    }
    if (!($junit_remove_prefix eq "")) {
        $junit_args = "$junit_args --remove-prefix=$junit_remove_prefix"
    }
    if (!($suppressions eq "")) {
        $junit_args = "$junit_args --suppress=$suppressions";
    }
    if ($junit_xml == 1) {
        print "[Generating jUnit XML report]\n";
        systemd("$pwd/../util/test/convert_start_test_log_to_junit_xml.py $junit_args");
    }
}


# Signal that all nodes are free to do some work by writing their
# synchronization files for them initially.
sub nodes_free {
    local ($id, $node, $fname, $dirv);

    print "making all nodes available to work\n" if $debug;
    # clean synch dir
    opendir WORKDIR, "$synchdir";
    @dirv = readdir WORKDIR;
    closedir WORKDIR;
    foreach $file (@dirv) {
        unlink "$synchdir/$file";
    }

    systemd ("rm -f $synchdir/*");
    # signal that all nodes are free
    my $nodeCount = $#node_list + 1;
    for ($id=0; $id<$nodeCount; $id++) {
        $fname = "$node_list[$id].$id";
        systemd ("echo feed me > $synchdir/$fname");
    }
}


# Return true if a *.chpl or *.test.c exists. Otherwise false.
sub chpl_files {
    local (@fnames) = @_;
    local ($found);
    $found = 0;
    foreach $fname (@fnames) {
        if ($fname eq "NOTEST") {
            return 0;
        }
        if ($fname =~ /\.chpl$/     ||
	    $fname =~ /\.test\.c$/  ||
	    $fname eq "sub_test"
	) {
	    # still a NOTEST would override
            $found = 1;
        }
    }
    return $found;
}


# Gather all the subdirectories into a flat list and return it.
sub find_subdirs {
    local ($targetdir, $level) = @_;
    local ($filen, @cdir, @founddirs, $i);

    print "looking in '$targetdir'\n" if $debug;
    opendir CURRDIR, $targetdir or die "Cannot open directory '$targetdir'\n";
    @cdir = grep !/^\./, readdir CURRDIR;             # curr dir list of files
    closedir CURRDIR;

    if (chpl_files (@cdir)) {                         # if *.chpl files in dir
	push @founddirs, $targetdir;
	print "$targetdir\n" if $debug;
    }
    
    foreach $filen (@cdir) {
	next if ($filen =~ /$dirs_to_ignore/);
	if (-d "$targetdir/$filen") {                 # if dir
        # .skipif does double duty here.  
        # When attached to a directory name, skips it and all descendents if the condition is true.
        my $skipfilen = "$targetdir/$filen.skipif";
        if (-e "$skipfilen") {
            my $skip;
            if (-x "$skipfilen") {
                print "$skipfilen = " . `$skipfilen` if $debug;
                $skip = `$skipfilen`;
            } else {
                print "$ENV{CHPL_HOME}/util/test/testEnv $skipfilen = " . `$ENV{CHPL_HOME}/util/test/testEnv $skipfilen` if $debug;
                $skip = `$ENV{CHPL_HOME}/util/test/testEnv $skipfilen`;
            }
            next if $skip > '0' or $skip =~ m/true/i;
        }
	    if ($debug) {for ($i=0; $i<$level; $i++)  {print "    ";}}
	    push @founddirs, find_subdirs ("$targetdir/$filen", $level+1);
        }
    }
    return @founddirs;
}


# Gather the list of files to test and return it.
sub find_files {
    local ($targetdir, $level, $no_futures, $recursive) = @_;
    local ($filen, @cdir, @foundfiles);

    print "looking in '$targetdir'\n" if $debug;
    opendir CURRDIR, $targetdir or die "Cannot open directory '$targetdir'\n";
    @cdir = grep !/^\./, readdir CURRDIR;             # curr dir list of files
    closedir CURRDIR;

    foreach $filen (@cdir) {
	next if ($filen =~ /$dirs_to_ignore/);
	$filepath = "$targetdir/$filen";
	unless (-e "$targetdir/NOTEST") {             # do not ignore this dir?
	    if ($filepath =~ /\.chpl$/) {
		$futuref = $filepath;
		$futuref =~ s/\.chpl$/.future/;
		next if ((-e $futuref) && $no_futures);
		push @foundfiles, $filepath;
	    } elsif ($filepath =~ /\.test\.c$/) {
		$futuref = $filepath;
		$futuref =~ s/\.test\.c$/.future/;
		next if ((-e $futuref) && $no_futures);
		push @foundfiles, $filepath;
	    }
	}

	if ((-d $filepath) && $recursive) {           # if dir and recursive
	    if ($debug) {for ($i=0; $i<$level; $i++)  {print "    ";}}
	    push @foundfiles, find_files ($filepath, $level+1, $no_futures, $recursive);
        }
    }
    return @foundfiles;
}


sub print_help {
    print "Usage: paratest.server [-compopts s] [-dirfile d] [-execopts s] [-filedist] [-futures] [-logfile l] [-memleaks f] [-nodefile n] [-nodepara m] [-suppress f] [-valgrind] [-help|-h] [-timeout t]\n";
    print "    -compopts s: s is a string that is passed with -compopts to start_test.\n";
    print "    -dirfile  d: d is a file listing directories to test. Default is the current diretory.\n";
    print "    -execopts s: s is a string that is passed with -execopts to start_test.\n";
    print "    -filedist  : distribute work at the granularity of files (directory granurality is the default).\n";
    print "    -futures   : include .future tests (default is none).\n";
    print "    -logfile  l: l is the output log file. Default is \"user\".\"platform\".log. in the Logs subdirectory.\n";
    print "    -memleaks f: pass -memleaks to start_test; aggregate all output.\n";
    print "    -nodefile n: n is a file listing nodes to run on. Default is current node.\n";
    print "    -nodepara m: Run m paratest.client tasks on each node.\n";
    print "    -suppress f: f is a file with the tests to suppress using filterSuppressions.\n";  
    print "    -valgrind  : pass -valgrind to start_test.\n";
    print "    -timeout  t: t is the max time to wait after last directory is served.\n";
    print "    -junit-xml : Create jUnit test report.\n";
    print "    -junit-xml-file f: Put jUnit test report at location 'f' (implies -junit-xml).\n";
    print "    -junit-remove-prefix p: Remove prefix 'p' from all tests in junit report.\n";
}


sub main {
    local ($id, $synchfile);
    
    $user = `whoami`; chomp $user;
    $platform = `../util/chplenv/chpl_platform.py`; chomp $platform;
    $fin_logfile = "$logdir/$user.$platform.log";      # final log file name
    # $fin_logfile = "$logdir/$user.$platform.log";
    unlink $fin_logfile if (-e $fin_logfile);          # remove final log file
  
    $starttime = `date`; chomp $starttime;

    while ($#ARGV >= 0) {
        $_ = $ARGV[0];
        if (/^-compopts/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $compopts = "$compopts $ARGV[0]";
            } else {
                print "missing -compopts arg\n";
                exit (8);
            }
        } elsif (/^-execopts/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $execopts = "$execopts $ARGV[0]";
            } else {
                print "missing -execopts arg\n";
                exit (8);
            }
        } elsif (/^-filedist/) {
            $filedist = 1;
        } elsif (/^-dirfile/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $dirfile = $ARGV[0];
            } else {
                print "missing -dirfile arg\n";
                exit (8);
            }
         # "undocumented" option, only meant to be used by other scripts
        } elsif (/^-futures-mode/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $futures_mode = $ARGV[0];
            } else {
                print "missing -futures-mode arg\n";
                exit (8);
            }
        } elsif (/^-futures/) {
            $futures_mode = 1;
        } elsif (/^-logfile/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $fin_logfile = $ARGV[0];
            } else {
                print "missing -logfile arg\n";
                exit (8);
            }
        } elsif (/^-suppress/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $suppressions = $ARGV[0];
            } else {
                print "missing -suppress arg\n";
                exit (8);
            }
        } elsif (/^-nodefile/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $nodefile = $ARGV[0];
            } else {
                print "missing -nodefile arg\n";
                exit (8);
            }
        } elsif (/^-nodepara/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $node_para = $ARGV[0];
            } else {
                print "missing -nodepara arg\n";
                exit(8);
            }
        } elsif (/^-timeout/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $timeout = $ARGV[0];
            } else {
                print "missing -timeout arg\n";
                exit(8);
            }
        } elsif (/^-memleaks/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $memleaks = $ARGV[0];
                $memleaksflag = 1;
            } else {
                print "missing -memleaks arg\n";
                exit (8);
            }
        } elsif (/^-valgrind/) {
            $valgrind = 1;
        } elsif (/^-show-all-errors/) {
            $show_all_errors = 1;
        } elsif (/^-junit-xml$/) {
            $junit_xml = 1;
        } elsif (/^-junit-xml-file/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $junit_xml_file = $ARGV[0];
                $junit_xml = 1;
            } else {
                print "missing -junit-xml-file arg\n";
                exit (8);
            }
        } elsif (/^-junit-remove-prefix/) {
            shift @ARGV;
            if ($#ARGV >= 0) {
                $junit_remove_prefix = $ARGV[0];
            } else {
                print "missing -junit-remove-prefix arg\n";
                exit (8);
            }
        } elsif (/^-help|^-h/) {
            print_help;
            exit (9);
        } else {
            print "unknown arg $_\n";
            exit (9);
        }
        shift @ARGV;
    }

    die "Error: CHPL_HOME must be set\n" unless defined $ENV{CHPL_HOME};

# Set some more environment variables so they can be tested by skipif
    $ENV{'COMPOPTS'} = $compopts;
    $ENV{'EXECOPTS'} = $execopts;

    if (defined $nodefile) {
        open nodefile or die "Cannot open node file '$nodefile'\n";
        while (<nodefile>) {
            next if /^$|^\#/;
            chomp;
            $count = $node_para;
            while ($count--) {
                push @node_list, $_;
            }
        }
    } else { # else, just current node
        if( defined $ENV{'SLURM_JOB_NODELIST'} ) {
            my $SLURM_NODELIST = $ENV{'SLURM_JOB_NODELIST'};
            my $SLURM_NODES = `scontrol show hostnames $SLURM_NODELIST`;
            my @SLURM_NODES = split(' ', $SLURM_NODES);
            for my $node (@SLURM_NODES) {
                print "using node $node from SLURM\n";
                $count = $node_para;
                while ($count--) {
                    push @node_list, $node;
                }
            }
        } else {
            while ($node_para--) {
                push @node_list, $localnode;
            }
        }
    }

    if (defined $dirfile) {
        open dirfile or die "Cannot open directory file '$dirfile'\n";
        while (<dirfile>) {
            next if /^$|^\#/;
            chomp;
            if ($filedist) {
                push @testdir_list, find_files( $_, 0, !$futures_mode, 0);
            } else {
                push @testdir_list, $_;
            }
        }
    } else { # else, current working dir
        use Cwd;
        my $cwd = &Cwd::cwd();
        print "[Generating tests from the Chapel Spec in $ENV{CHPL_HOME}/spec]\n";
        chdir $ENV{CHPL_HOME} or die "Can't cd to $ENV{CHPL_HOME}: $!\n";
        my $autogen=`make spectests`;
        die "Error generating Spec tests in $ENV{CHPL_HOME}/spec\n" unless $? == 0;
        chdir $cwd;
        if ($filedist) {
	    print "[Collecting test files in $cwd]\n";
            @testdir_list = find_files (".", 0, !$futures_mode, 1);
        } else {
	    print "[Collecting test directories in $cwd]\n";
            @testdir_list = find_subdirs (".", 0);
        }
    }

    unless (-e $logdir) {
        print "Error: log directory $logdir does not exist\n";
        exit (2);
    }
    unless (-e "$synchdir") { 
        systemd ("mkdir $synchdir"); 
    }

    # Package up the environment
    my $tmpchplenv = `$ENV{CHPL_HOME}/util/printchplenv --sh`;
    my $chplenv = "";
    my @lines = split(/\n/, $tmpchplenv);
    for my $line (@lines) {
        $line =~ s/export\s+//;
        my ($key,$value) = split(/=/, $line, 2);
        # Now remove the single quotes in the value.
        if( $value =~ /^'.+'$/ ) {
            $value =~ s/^'//;
            $value =~ s/'$//;
        }
        $chplenv .= "$key=$value ";
    }

    nodes_free ();         # signal that all nodes free
    feed_nodes ($chplenv); # parallel testing

    # cleanup - remove synch files and synch dir
    for ($id=0; $id<=$#node_list; $id++) {
        $synchfile = "$synchdir/$node_list[$id].$id";
        unlink $synchfile if (-e $synchfile);
    }
    rmdir $synchdir;

    # generate jUnit report
    generate_junit_xml();
}


main ();

