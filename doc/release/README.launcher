================
Chapel launchers
================

When compiling Chapel programs for multiple locales, a launcher binary
is typically created that will execute the appropriate command(s) to
get your program started.  For example, when compiling for multiple
locales, typically two binaries will be generated by the compiler
(e.g., a.out and a.out_real).  The first binary contains code to get
your program up and running on multiple locales while the second
contains your actual program code.

The goals of the launcher binary are: 

(1) to wrap details of job startup in a portable way so that new users
    can quickly get Chapel programs up and running on an unfamiliar
    platform.

(2) to perform command-line parsing and error checking prior to
    waiting in a queue or firing off a parallel job in order to save
    time and resources related to simple errors/typos in the command
    line.

(3) to preserve Chapel's global-view programming model by permitting
    the user to run their program using a single binary (corresponding
    to the single logical task that executes main()) without getting
    bogged down in questions of numbers of nodes, numbers of cores per
    node, numbers of program instances to start up, etc.

(4) if necessary, to coordinate runtime functional activity, such as
    I/O. 

Executing a Chapel program using the verbose (-v) flag will typically
print out the command(s) used to launch the program.  

Executing using the help (-h/--help) flag will typically print out any
launcher-specific options in addition to the normal help message for
the program itself.

Currently-supported launchers include:

  amudprun            : GASNet launcher for programs running over UDP
  aprun               : Cray XT application launcher using aprun
  gasnetrun_ibv       : GASNet launcher for programs running over Infiniband
  loadleveler         : launch using IBM loadleveler (still needs refining)
  lsf-gasnetrun_ibv   : GASNet launcher using LSF (bsub) over Infiniband
  marenostrum         : launch using MareNostrum's mnsubmit script
  mtarun              : Cray XMT application launcher using mtarun
  pbs-aprun           : Cray XT application launcher using PBS (qsub) + aprun
  pbs-gasnetrun_ibv   : GASNet launcher using PBS (qsub) over Infiniband
  slurm-gasnetrun_ibv : GASNet launcher using slurm (sbatch) over Infiniband
  tile-monitor        : launch using Tilera's tile monitor
  none                : do not use a launcher

A specific launcher can be requested by setting the CHPL_LAUNCHER
environment variable.  If left unset, a default is picked as follows:

  CHPL_PLATFORM=cray-cascade, cray-xe, cray-xk, or cray-xt.
  + qsub & aprun in user's path -> qsub-aprun
  + only aprun in user's path   -> aprun
  (otherwise)                   -> none

  CHPL_PLATFORM=xmt             -> mtarun

  CHPL_PLATFORM=marenostrum     -> marenostrum

  CHPL_COMM=gasnet...
  + CHPL_COMM_SUBSTRATE=ibv
    + CHPL_TARGET_PLATFORM=pwr6 -> none
    (otherwise)                 -> gasnetrun_ibv
  + CHPL_COMM_SUBSTRATE=udp     -> amudprun
  (otherwise)                   -> none

  CHPL_TARGET_COMPILER=tile-cc  -> tile-monitor

  (otherwise)                   -> none

If the launcher binary does not work for your system (due to an
installation-specific configuration, e.g.), you can often use the -v
flag to capture the commands that the launcher executes on your behalf
and customize them for your needs.  


---------------------------------
_real binary suffix for execution
---------------------------------

In order to support profiling tools that produce new binaries for the
launcher to execute, the suffix of the real binary executed by the
launcher may be changed with the CHPL_LAUNCHER_SUFFIX environment
variable. If this variable is unset, the suffix defaults to "_real",
matching the compiler's output.


----------------------
Bypassing the launcher
----------------------

If the Chapel launcher capability fails you completely, set
CHPL_LAUNCHER to none, recompile, and execute the a.out binary
according to the following rules using tools and queueing mechanisms
appropriate for your system:

* on most systems, the number of locales should be equal to the number
  of nodes on which you execute which should be equal to the number of
  copies of a.out (or a.out_real using a launcher) that you are
  running.

* some queueing systems require you to specify the number of cores to
  use per node.  For best results, you will typically want to use all
  of them.  All intra-node parallelism is typically implemented using
  Chapel's threading layer (e.g., pthreads), so extra copies of the
  binary are not required per core.

----------------------
Slurm notes
----------------------
You can request a constraint to slurm if needed by putting it in the
CHPL_LAUNCHER_CONSTRAINT environment variable, for example, to use the cal
nodes as defined in slurm you need to define it as :

export CHPL_LAUNCHER_CONSTRAINT=cal

If not defined any node in the computer can be used.

Right now it sends the job to the queue system, so it appears when using
squeue, and will write the output in a file called name_of_the_exec.jobId.out

The variable CHPL_LAUNCHER_USE_SRUN is reserved for when a new launch method
gets implemented, so it will be easy to change between the different launch
methods.  Meanwhile it must be undefined.

