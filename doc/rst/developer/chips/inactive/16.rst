.. _record-copies-expiring:

Optimizing Record and Array Copies in Chapel
============================================

Status:
  Inactive - Partially Implemented with copy-elision and split-init

Authors:
  Michael Ferguson

Abstract
--------

This CHIP describes two strategies for optimizing record copies.

Rationale
---------

Record and array copies can be expensive. Since a record copy executes
a copy initializer, it can be arbitrarily costly. Array copies can be
expensive because the arrays involved can be arbitrarily large.

This document describes how these copies can be optimized while still
presenting a reasonable programming model for Chapel users.

Note that arrays and records have similar copy behavior in Chapel.

Description
-----------

The design in :ref:`record-copies` can be extended to generally eliminate
unnecessary copies.

Improving In Argument Intent
++++++++++++++++++++++++++++

The `in` argument intent should behave similarly to variable
initialization. In particular, in the following example, there is no need
to copy the data from `makeArray` into `sink`:

.. code-block:: chapel

    proc makeArray() {
      var A:[1..100] int;
      return A;
    }

    proc sink(in a) {
      writeln(a);
    }

    sink(makeArray());


Initializers Should Generally Use In Intent
+++++++++++++++++++++++++++++++++++++++++++

When initializing a record, arguments to the initializer - especially the
compiler provided default initializer - are generally turned into fields.
If such arguments have the `in` argument intent, then optimization is
enabled:


.. code-block:: chapel

    record R {
      var A:[1..100] int;
    }
    proc makeArray() {
      var A:[1..100] int;
      return A;
    }
    var r = new R(makeArray());

When `new R( :R)` takes in its argument with the `in` intent,
optimization is enabled because the argument can be `move initialized`
with the result of `makeArray`.

The copy initializer should not use the `in` argument intent.
(In fact it would probably be an error to do so because implementing
the `in` argument intent requires copy initialization)

Eliminating Copies from Expiring Values
+++++++++++++++++++++++++++++++++++++++

There are several cases that motivate an optimization that transforms
copy-initialization from an expiring value into move initialization.

The `out` argument intent should behave similarly to returning a variable
from a function. In particular, in the following example, there is no
need to copy the array A in order to get its value into B:

.. code-block:: chapel

    proc outExample(out arg) {
      var A:[1..100] int;
      arg = A;
    }

    var B:[1..100] int;
    outExample(B);

Here is a further example that combines `in` argument intent and `out`
argument intent. This example also does not need any copies to move
the data from the `makeArray` calls in `source` to `sink`:

.. code-block:: chapel

    proc makeArray() {
      var A:[1..100] int;
      return A;
    }

    proc source(out a, out b) {
      a = makeArray();
      b = makeArray();
    }

    proc sink(in a, in b) {
      writeln(a);
      writeln(b);
    }

    var tmp_a:A[1..100] int;
    var tmp_b:A[1..100] int;
    source(tmp_a, tmp_b);
    sink(tmp_a, tmp_b);

One issue here is that it is not possible to pass the `out` arguments to
`in` arguments without going through a named user variable. However the
variables `tmp_a` and `tmp_b` are expiring and so are subject to further
optimization.

In addition, a program such as this:

.. code-block:: chapel

  {
    var A:[1..4] int;
    var B = A;
    var C = B;
    writeln(C);
  }

creates an extra copy of A that is not needed.


Here we propose that:

 1) the compiler can remove any `copy` if the source of the copy
    is an expiring value (that is, a value that is dead after the copy
    is made except for a call to destroy it - this call to destroy
    it is required ).
 2) when the compiler removes such `copy` operations, it replaces them
    with `move` operations.

There is one case where this behavior might be surprising to a record
author. Suppose that `R` is a record that contains a `ptr` field of a class
type. Suppose a copy initializer is defined for `R` that allocates a
new `ptr` value in the destination and copies the contents of `ptr`.
Suppose further that a `move` initialization for `R` does not take
any special action (ie is equivalent to a shallow copy).

Now suppose that `g` is an `R` record value storing a pointer, and that
somehow `alias` is set up as a record storing the same pointer.  Then a
copy is made from `alias` to `x`. Finally, the value pointed to within
`x` is modified.

.. code-block:: chapel

    {
      var g = new R(ptr);
      var alias:R;
      alias.ptr = g.ptr;
      var x = alias; // a copy might be expected here.
                     // if the copy occurs, x.ptr != g.ptr.
                     // if it does not, x.ptr == g.ptr.
      mutate(x.ptr);
      // has g.ptr changed?
    }

In this case, the optimization might remove the copy from `alias` to `x`,
which would cause the mutate call to modify `g.ptr` instead of a separate
value. This difference changes the way the program behaves. Note that it
is also possible to write this pattern as several function calls so that
the role played by `x` is instead played by a compiler-introduced
temporary.

Here we propose that in cases where a record might store a pointer that
aliases another record, the `move` initialization needs to be adjusted by
the record author to copy the data. This is already the case for arrays
with the `unalias` function.

.. comment
  This optimization still meets the *unique storage for a record's fields*
  idea from :ref:`record-copies-user-view` since the optimized-away copy is
  from a dead variable.

Alternative designs include:

 * apply this optimization only to compiler-introduced temporaries
   and always apply it to compiler-introduced temporaries
   (This is the choice that C++ and D made, but it has the disadvantage
   that user variables have different behavior from compiler-introduced
   temporaries - and that this optimization cannot apply to user
   variables).
 * apply this optimization only when the result of a possibly-eliminated
   copy is not logically modified
   (This choice is possible but would require an understanding
   of `const` or some other concept that includes mutation through
   a pointer field, such as transitive immutability).

Optimizing Assignment
+++++++++++++++++++++

In certain cases, such as with the `bigint` record, it is common
to have assignment from a temporary value. For example, this program:

.. code-block:: chapel

  var a = new bigint(1);
  var b = new bigint(2);
  var c:bigint;

  c = a + b;

will copy the temporary result of `a + b` into `c`. But this program is
equivalent to the following program in which no such copy occurs:

.. code-block:: chapel

  var a = new bigint(1);
  var b = new bigint(2);

  var c = a + b;

It is possible and reasonable to optimize these assignment operations but
it will not make sense for all possible record types. In particular, one
challenging case is assignment for records that implement reference
counting where the left-hand-side and the right-hand-side point to the
same data. Any optimization strategy needs to allow such a
reference-conting record type to be built.

When using reference counting in Objective C for example, the assignment
operation for a reference-counted field normally follows the discipline
of doing a `retain` before a `release`. The `retain` increments the
reference count and the `release` decrements it. In this way, if the
field is set to its original value, the reference count is increased and
then decreased back to its original value without any other effect.
Compare that with the `release` and then `retain` ordering - where the
reference count might reach 0 from the `release` before the `retain` has
a chance to set it back to 1. That is a problem because the field could
be freed in that case.

So, for a reference-counting record in Chapel, we would expect that the
record author would need to create an assignment operation like so:

.. code-block:: chapel

  proc =(ref lhs:refcnt, rhs:refcnt) {
    rhs.refcount.increment();
    lhs.refcount.decrement();
    lhs.refcount = rhs.refcount;
    lhs.value = rhs.value;
  }

Thus, the assignment operation is complicated.

Compare that with a bigint assignment function which could be implemented
as:

.. code-block:: chapel

  proc =(ref lhs:bigint, rhs:bigint) {
    lhs.~bigint(); // destroy lhs
    var tmp = rhs; // copy-initialize tmp from rhs
    // now shallow-copy from tmp to lhs
    lhs.mpz = tmp.mpz; // move over pointer
    lhs.localeId = tmp.localeId; // move over locale
    // now clear out tmp so it is not destroyed
    tmp.mpz = nil;
    tmp.localeId = 0;
  }

We would like for assignment functions such as this bigint assignment
function to enable the translation of assignment into a pattern more
similar to copy initialization.

The proposal here is to make the default, compiler-generated assignment
function work as the bigint example above does. In particular it:

 * destroys the LHS
 * copy-initializes from the RHS into the LHS

Now, if a record has any field that uses a different (non-default)
assignment function, the compiler-generated assignment function would
switch to a strategy calling the assignment function on each field in
turn.

And so we have two assignment function possibilities. A record might have
an assignment function that amounts to destroy-copy or it might have one
that is more complicated. And the default assignment function for a record
could be in either category.

Now, for a record with a destroy-copy assignment function, the compiler
can inline that operation. Then it can apply the expiring value
optimization described above. The result is optimization of assignment.

Let's consider our original example:

.. code-block:: chapel

  var a = new bigint(1);
  var b = new bigint(2);
  var c:bigint;

  c = a + b;

Since bigint will have a destroy-copy assignment function, the compiler
will translate this example into the following:

.. code-block:: chapel

  var a = new bigint(1);
  var b = new bigint(2);
  var c:bigint;

  c.~bigint();
  c.init(a + b, dst); // ie c is copy initialized from a+b

Now, since the result of a+b is stored into a (compiler-introduced)
temporary variable, it is an expiring value. As a result, the expiring
value optimization above applies and c is `move initialized` with the
result of `a+b`.

Optimizing Array Assignment Across Locales
++++++++++++++++++++++++++++++++++++++++++

Array assignment across locales can be optimized when the array element
type uses a destroy-copy assignment function.

In particular, the array assignment would do the following:

 * destroy the elements of the LHS array
 * overwrite the destroyed LHS elements with shallow copies of the LHS elements
   (ie do the communication. This could be done in single GET.)
 * copy-initialize each LHS element from itself

The array assignment would fall back on element-by-element assignment
in the case that the element type does not use a destroy-copy assignment
function.

