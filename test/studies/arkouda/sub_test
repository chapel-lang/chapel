#!/usr/bin/env bash

# Custom sub_test to run Arkouda testing. Clones, installs dependencies, builds
# Arkouda and runs testing.

ARKOUDA_URL=${ARKOUDA_URL:-https://github.com/Bears-R-Us/arkouda.git}
ARKOUDA_BRANCH=${ARKOUDA_BRANCH:-main}

export CHPL_TEST_ARKOUDA_STOP_AFTER_BUILD=${CHPL_TEST_ARKOUDA_STOP_AFTER_BUILD:-"false"}

CWD=$(cd $(dirname ${BASH_SOURCE[0]}) ; pwd)
source $CWD/functions.bash

subtest_start

DFLT_TIMEOUT=${CHPL_TEST_TIMEOUT:-300}
export ARKOUDA_CLIENT_TIMEOUT=${ARKOUDA_CLIENT_TIMEOUT:-$DFLT_TIMEOUT}

ARKOUDA_ALLOC_TIMEOUT=10:00:00

if [ -n "${CHPL_TEST_LAUNCHCMD}" ] ; then
  export ARKOUDA_SERVER_LAUNCH_PREFIX=$(echo "$CHPL_TEST_LAUNCHCMD --walltime=$ARKOUDA_ALLOC_TIMEOUT" | envsubst)
fi

export ARKOUDA_PYTEST_OPTIONS=${CHPL_TEST_ARKOUDA_CORRECTNESS_PYTEST_OPTIONS:-""}

# Arkouda needs chpl in PATH
bin_subdir=$($CHPL_HOME/util/chplenv/chpl_bin_subdir.py)
export "PATH=$CHPL_HOME/bin/$bin_subdir:$PATH"
chpl --version

export ARKOUDA_HOME=$CWD/arkouda

if [ -z "${CHPL_TEST_ARKOUDA_SKIP_RM_CLONE}" ] ; then
  rm -rf ${ARKOUDA_HOME}

  # Clone Arkouda 
  log "cloning Arkouda (URL=$ARKOUDA_URL, branch=$ARKOUDA_BRANCH"
  if ! git clone --depth=1 ${ARKOUDA_URL} --branch=${ARKOUDA_BRANCH} ; then
    log_fatal_error "cloning Arkouda"
  fi
fi

cd ${ARKOUDA_HOME}

# report the current git hash
git_hash=$(git rev-parse HEAD)
# strip .git from the end of ARKOUDA_URL, if it's there
git_url=$(echo $ARKOUDA_URL | sed 's/\.git$//')
log "Current Arkouda commit is $git_hash, see $git_url/commit/$git_hash"

if [ -z "${ARKOUDA_SKIP_CHECK_DEPS}" ] ; then
  # Install dependencies if needed
  if make check-deps ; then
    export ARKOUDA_SKIP_CHECK_DEPS=true
  else
    if ! nice make -j $($CHPL_HOME/util/buildRelease/chpl-make-cpu_count) install-deps ; then
      log_fatal_error "installing dependencies"
    fi
  fi
else
  log "Skipping dependency checks"
  log "Dependency dir: $ARKOUDA_DEP_DIR"
fi

export "PATH=${ARKOUDA_HOME}/dep/hdf5-install/bin:$PATH"

# CHPL_TEST_ARKOUDA_DISABLE_MODULES is a colon separated list of modules to
# disable.  Disable these modules by commenting them out in ServerModules.cfg
if [ -n "${CHPL_TEST_ARKOUDA_DISABLE_MODULES}" ] ; then
  IFS=":"
  for mod in $CHPL_TEST_ARKOUDA_DISABLE_MODULES; do
    cmd="s/^\s*$mod/#$mod/g"
    sed -i'' -e $cmd ServerModules.cfg
  done
  unset IFS
fi

# install frontend python bindings
if [ -z "${CHPL_TEST_ARKOUDA_SKIP_CHAPEL_PY}" ] ; then
  (cd $CHPL_HOME && make chapel-py-venv)
fi

# Enable multi dim array support
if [ -n "${CHPL_TEST_ARKOUDA_MULTI_DIM}" ] ; then
  apply_git_patch $CHPL_HOME/test/studies/arkouda/multidim.patch $ARKOUDA_HOME
fi

# Compile Arkouda
if [ "${CHPL_TEST_ARKOUDA_PERF}" = "true" ] ; then
  PERF_SUB_DIR="$CHPL_TEST_PERF_DIR/$CHPL_TEST_PERF_DESCRIPTION"
  mkdir -p "$PERF_SUB_DIR"
  export ARKOUDA_PRINT_PASSES_FILE="$PERF_SUB_DIR/comp-time"
  export ARKOUDA_EMITTED_CODE_SIZE_FILE="$PERF_SUB_DIR/emitted-code-size"
  export CHPL_DEBUG_FLAGS="${CHPL_DEBUG_FLAGS} --print-emitted-code-size"

  if [ -z "${CHPL_TEST_ARKOUDA_SKIP_BUILD}" ] ; then
    make 2>&1 | tee $ARKOUDA_EMITTED_CODE_SIZE_FILE.tmp
    if [ ${PIPESTATUS[0]} -ne "0" ] ; then
      top bn1 2>/dev/null || echo "'top' failed (ignored)" # check for "competitors" for memory etc.
      log_fatal_error "compiling arkouda"
    fi
    if grep -q "Statements emitted:" $ARKOUDA_EMITTED_CODE_SIZE_FILE.tmp ; then
      grep "Statements emitted:" $ARKOUDA_EMITTED_CODE_SIZE_FILE.tmp > $ARKOUDA_EMITTED_CODE_SIZE_FILE
      rm -f $ARKOUDA_EMITTED_CODE_SIZE_FILE.tmp
    fi
  fi
else
  if [ -z "${CHPL_TEST_ARKOUDA_SKIP_BUILD}" ] ; then
    if ! make ; then
      log_fatal_error "compiling arkouda"
    fi
  fi
fi

if [ ${CHPL_TEST_ARKOUDA_STOP_AFTER_BUILD} = "false" ]; then

  # If Arkouda deps use any of our test deps, try to use the versions we want
  AK_PIP_CONTRAINTS="--constraint $CHPL_HOME/third-party/chpl-venv/test-requirements.txt"
  # create a virtual environment for running the tests
  # if one was already created and set in CHPL_TEST_VENV_DIR, use it
  if [ -n "$CHPL_TEST_VENV_DIR" ] && [ "$CHPL_TEST_VENV_DIR" != "none" ]; then
    log "Using the existing virtual environment for Arkouda at '$CHPL_TEST_VENV_DIR'"
    source $CHPL_TEST_VENV_DIR/bin/activate
  else
    ARKOUDA_TEST_VENV=.arkouda-nightly-test-venv
    log "Removing the virtual environment (if it exists) in $ARKOUDA_TEST_VENV"
    rm -rf $ARKOUDA_TEST_VENV
    log "Creating a new virtual environment for Arkouda using '$(which python3)'"
    python3 -m venv $ARKOUDA_TEST_VENV
    source $ARKOUDA_TEST_VENV/bin/activate
  fi

  log "Installing Arkouda in the virtual environment using '$(which python3)'"
  if ! python3 -m pip install --force-reinstall --timeout 60 $AK_PIP_CONTRAINTS -e .[dev] ; then
    log_fatal_error "installing arkouda"
  else
    log "Use 'source $(pwd)/$ARKOUDA_TEST_VENV/bin/activate' to activate the venv"
  fi

  # Check installation
  if [ -z "${CHPL_TEST_ARKOUDA_SKIP_MAKE_CHECK}" ] ; then
    test_start "make check"
    if make check ; then
      log_success "make check output"
    else
      log_fatal_error "running make check"
    fi
    test_end
  fi

  # Run Python unit tests
  if [ -z "${CHPL_TEST_ARKOUDA_SKIP_UNIT_TESTS}" ] ; then
    test_start "make test-python"
    if make test-python ; then
      log_success "make test-python output"
    else
      log_error "running make test-python"
    fi
    test_end
  fi

  # Run benchmarks
  if [ "${CHPL_TEST_ARKOUDA_PERF}" = "true" ] ; then
    benchmark_opts="--save-data --dat-dir $CHPL_TEST_PERF_DIR --gen-graphs --graph-dir $CHPL_TEST_PERF_DIR/$CHPL_TEST_PERF_DESCRIPTION/html"
    if [ "${CHPL_TEST_GEN_ARKOUDA_GRAPHS}" = "false" ] ; then
        # Where should perf logs go?
        export ARKOUDA_TEST_PERF_DIR="${WORKSPACE}/perfData"
        test_start "make perf dir"
        if mkdir -p $ARKOUDA_TEST_PERF_DIR ; then
            log_success "created perf directory $ARKOUDA_TEST_PERF_DIR"
        else
            log_error "creating perf directory"
        fi
        test_end
        benchmark_opts="--save-data --dat-dir $ARKOUDA_TEST_PERF_DIR"
    fi

    benchmark_opts="${benchmark_opts} --annotations $CHPL_HOME/test/ANNOTATIONS.yaml"
    if [[ -n $CHPL_TEST_PERF_DESCRIPTION ]]; then
        if [ "${CHPL_TEST_GEN_ARKOUDA_GRAPHS}" = "true" ] ; then
            benchmark_opts="${benchmark_opts} --description $CHPL_TEST_PERF_DESCRIPTION"
        fi
    fi
    if [[ -n $CHPL_TEST_PERF_CONFIG_NAME ]]; then
      benchmark_opts="${benchmark_opts} --platform $CHPL_TEST_PERF_CONFIG_NAME"
    fi
    if [[ -n $CHPL_TEST_NUM_TRIALS ]]; then
      benchmark_opts="${benchmark_opts} --numtrials $CHPL_TEST_NUM_TRIALS"
    fi
    if [[ -n $CHPL_TEST_PERF_START_DATE ]]; then
      benchmark_opts="${benchmark_opts} --start-date $CHPL_TEST_PERF_START_DATE"
    fi
    if [[ -n $CHPL_TEST_PERF_CONFIGS ]]; then
      benchmark_opts="${benchmark_opts} --configs $CHPL_TEST_PERF_CONFIGS"
    fi
    if [[ -n $CHPL_TEST_ARKOUDA_BENCHMARKS ]]; then
      benchmark_opts="${benchmark_opts} $CHPL_TEST_ARKOUDA_BENCHMARKS"
    fi

    test_start "benchmarks"
    log "Running arkouda benchmarks with options: $benchmark_opts"
    if ./benchmarks/run_benchmarks.py ${benchmark_opts} ; then
      log_success "benchmark output"
    else
      log_error "running benchmarks"
    fi
    test_end
  fi

  # Run benchmarks_v2
  if [ "${CHPL_TEST_ARKOUDA_PERF_V2}" = "true" ] ; then
    PERF_SUB_DIR="$CHPL_TEST_PERF_DIR/$CHPL_TEST_PERF_DESCRIPTION"
    mkdir -p "$PERF_SUB_DIR"
    reformat_benchmark_opts=" --dat-dir $PERF_SUB_DIR --graph-dir $PERF_SUB_DIR/html"
    if [ "${CHPL_TEST_GEN_ARKOUDA_GRAPHS}" = "false" ] ; then
      # v2 benchmarks do not support not generating graphs
      log_fatal_error "CHPL_TEST_GEN_ARKOUDA_GRAPHS must be true for benchmarks_v2"
    fi
    reformat_benchmark_opts="${reformat_benchmark_opts} --annotations $CHPL_HOME/test/ANNOTATIONS.yaml"
    if [[ -n $CHPL_TEST_PERF_DESCRIPTION ]]; then
      reformat_benchmark_opts="${reformat_benchmark_opts} --description $CHPL_TEST_PERF_DESCRIPTION"
    fi
    if [[ -n $CHPL_TEST_PERF_CONFIG_NAME ]]; then
      reformat_benchmark_opts="${reformat_benchmark_opts} --platform-name $CHPL_TEST_PERF_CONFIG_NAME"
    fi
    if [[ -n $CHPL_TEST_PERF_START_DATE ]]; then
      reformat_benchmark_opts="${reformat_benchmark_opts} --start-date $CHPL_TEST_PERF_START_DATE"
    fi
    if [[ -n $CHPL_TEST_PERF_CONFIGS ]]; then
      reformat_benchmark_opts="${reformat_benchmark_opts} --configs $CHPL_TEST_PERF_CONFIGS"
    fi
    if [[ -n $CHPL_TEST_ARKOUDA_REFORMAT_BENCHMARKS_V2 ]]; then
      reformat_benchmark_opts="${reformat_benchmark_opts} $CHPL_TEST_ARKOUDA_REFORMAT_BENCHMARKS_V2"
    fi

    # TODO: skip io_benchmark since it has issues with ordering of file
    # reads/writes and gets lots of file-no-found errors
    sed '/\/io_benchmark.py/d' benchmark.ini > benchmark.ini.tmp && mv benchmark.ini.tmp benchmark.ini
    benchmark_size=$((10**8))

    benchmark_data=benchmark_v2/data/benchmark_stats_$(date +%Y%m%d_%H%M%S).json
    mkdir -p benchmark_v2/data
    benchmark_opts="--benchmark-autosave --benchmark-storage=file://benchmark_v2/.benchmarks --size=$benchmark_size --benchmark-json=$benchmark_data --benchmark-min-rounds=1 --benchmark-warmup=off"
    if [[ -n "$CHPL_TEST_ARKOUDA_TEMP_DIR" ]]; then
      benchmark_opts="${benchmark_opts} --basetemp=$CHPL_TEST_ARKOUDA_TEMP_DIR --io_path=$CHPL_TEST_ARKOUDA_TEMP_DIR/ak_io_benchmark "
    fi
    if [[ -n $CHPL_TEST_NUM_TRIALS ]]; then
      export ARKOUDA_NUM_PERF_TRIALS=$CHPL_TEST_NUM_TRIALS
      # TODO: for now, 1 trial no matter what
      export ARKOUDA_NUM_PERF_TRIALS=1
    fi
    if [[ -n $CHPL_TEST_ARKOUDA_BENCHMARKS_V2 ]]; then
      benchmark_opts="${benchmark_opts} $CHPL_TEST_ARKOUDA_BENCHMARKS_V2"
    fi

    log "Running arkouda benchmarks_v2 with options: $benchmark_opts"
    test_start "benchmarks_v2"
    if python3 -m pytest -c benchmark.ini $benchmark_opts ; then
      log_success "benchmark_v2 output"
    else
      log_error "running benchmarks_v2"
    fi

    log "Processing arkouda benchmarks_v2 data in dat-dir: $PERF_SUB_DIR"
    if python3 benchmark_v2/generate_field_lookup_map.py --dat-dir $PERF_SUB_DIR ; then
      log_success "benchmark_v2 field lookup map generation"
    else
      log_error "generating benchmark_v2 field lookup map"
    fi

    # process the data and create graphs
    log "Reformatting arkouda benchmarks_v2 data with options: $reformat_benchmark_opts"
    if python3 benchmark_v2/reformat_benchmark_results.py --benchmark-data $benchmark_data $reformat_benchmark_opts ; then
      log_success "benchmark_v2 graph generation"
    else
      log_error "generating benchmark_v2 graphs"
    fi

    test_end
  fi

  # deactivate the virtual environment
  deactivate

else
  # If we've gotten here then the build succeeded I need to create a "fake"
  # test for Jenkins even though in this mode all I really care about is the
  # build.
  test_start "build"
  log_success "build output"
  test_end
fi

subtest_end
