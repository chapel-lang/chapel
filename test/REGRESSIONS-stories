Guidance for REGRESSIONS week
=============================

> Our goal is to get rid of as many failures as possible this week,
  prioritizing those that affect users and developers (where a
  developer may be affected by...

  - getting frequent and not-useful mails from the nightly regressions
    due to sporadic failures

  - having to mentally filter out several dozen failures (e.g., mac
    developers)

  - having to go through a long list of tests to make sure that the
    REGRESSIONS file matches what we're seeing in a nightly run and
    that the world is as it should (whereas if every configuration had
    zero outstanding errors, this effort would be significantly lower)

> As in any development context, raise any discussion topics to the
  larger group (or a subset of it) before charging off in a direction
  that may be considered controversial, a change in the language or
  architecture, etc.

> You may also want to check in with the group before heading off on a
  particular story to see if there's any institutional knowledge that
  would be helpful.

> Generally speaking, many of these issues could be dealt with by
  adding a .skipif, but that should be the course of last resort.
  Here are some earlier-than-last resorts:

  - The first goal should be to try and preserve the original of the
    test if it makes sense in the given configuration, and to do what
    it takes to make the test exercise what it was meant to.  This
    could involve revamping or improving the test, dialing down the
    problem size, or taking a different approach to exercise the same
    thing.  I.e., there's no specific reason to feel beheld to the
    specifics of the test itself if it is fundamentally flawed.

  - If that's not possible, should we use a configuration-specific
    .good file to indicate that the test is working as we'd expect in
    this configuration, but won't match what was intended in the main
    .good?

  - If it doesn't make sense currently, but will at some point in the
    future (due to things beyond our control), using a suppression is
    a good approach (because we're notified when a failure that's
    meant to be suppressed is not found so will know when it becomes
    working).

  - If it doesn't make sense in a given configuration and never will,
    a .skipif is reasonable.

> In some cases, the same issue may be showing up in distinct forms in
  multiple configurations/tests.  As soon as you get a better
  diagnosis of one of the failure modes you're working on, it may be
  worthwhile broadcasting what you've learned so that if someone else
  is working on a related issue, you don't step on one anothers' toes.

> When looking for things to pick up, look for your name/email/obvious
  variations on these. Some issues I've assigned to people for
  historical or ownership reasons; others might be reasonable for you
  to pick up because you created the test (according to the pathname).

> Once you've cleared out anything with your name on it, look for
  topics near and dear to your heart/current work area (qthreads,
  string, memory, prgenv-cray, ...)  and see where that takes you
  (prioritizing high things over medium and medium over low,
  obviously.  In particular, I'd prefer that nobody work on medium/low
  priority things until all high items are owned or resolved).

> Each story has a title, a parenthetical indicating where it fails,
  and an owner if there is an obvious one, or "anyone" if not.  Each
  story correlates to one or more entries in the REGRESSIONS file --
  searching for the test name(s) in the REGRESSIONS file is the best
  way to correlate the two.

> I've sorted stories into high, medium, and low categories along with
  some indication of why they're in that category (e.g., some are high
  because they're noisy; others are high because they're scary; others
  xsare high because they're so easy).

> Helpful tip: When searching for a test in the
  chapel-test-results-all and/or -regressions mailing list archives
  (which can be a very useful tool to find recent past failures of a
  given kind), keep in mind that it uses a pretty naive "whole word
  searching" algorithm.  So don't search on "parSafeMember" if that's
  the name of the test you're looking for; instead, search for
  'domains/sungeun/assoc/parSafeMember]' (including the closing square
  bracket).  Also, note that the order in which results are returned
  from SourceForge's mailing list search is completely
  incomprehensible to me (give a shout if you have any insight into
  it).

> I could 60 stories which is 2/3 of where we started last time, and
  we're running lots more configurations now than then.  So while
  that's a higher number than we'd like, at least we're now worse off.
  This means if we could each close 5 stories, we'd have an absolutely
  clean testing run, retire REGRESSIONS and simply track issues in an
  issue tracker...


high priority (sources of noise)
--------------------------------

o sporadic timeout on tests that normally complete quickly (xc.*): elliot/gbt/anyone

  https://chapel.atlassian.net/browse/CHAPEL-2

  Specific tests aren't listed (which is not very helpful).  Is this
  the stream* case?  If anyone knows which tests are affected, please
  collapse any other stories below into this one as appropriate...


o sporadic dropped output (prgenv-cray): anyone but poor gbt who's
  spent enough time on this

  https://chapel.atlassian.net/browse/CHAPEL-3

  For the past several months, we've seen fairly consistent dropping
  of I/O in testing but so infrequently and across so many tests that
  it's hard to know how to triagulate on the issue and determine
  whether it's ours or CCE's, let alone reproduce it.  See the
  'sporadic dropping/mangling of otput' for details

  On one hand, this is one of our most annoying and disquieting
  sources of noise; on the other, it's maddeningly difficult to chase
  down, so time _may_ be best spent elsewhere...


o SSCA2_main times out sporadically (xe.ugni*, perf.xc.*): anyone

  https://chapel.atlassian.net/browse/CHAPEL-4

  Seems like we probably need to dial down the problem size in some fashion?

  - release/examples/benchmarks/ssca2/SSCA2_main


o miniMD timeouts (perf.xc.16.mpi.gnu, perf.xc.16.ugni.gnu): anyone/ben

  https://chapel.atlassian.net/browse/CHAPEL-5

  What would it take to solidify this test's execution time?


o ra times out sporadically (xe.ugni*): anyone

  https://chapel.atlassian.net/browse/CHAPEL-6

  Is this surprising?  Is it the same stream* timeout issue indicated
  above?

  - release/examples/benchmarks/hpcc/ra


o sporadic invalid read/write of size 8 in dl_* (valgrind): anyone

  https://chapel.atlassian.net/browse/CHAPEL-7

  Under valgrind testing, these tests fail with some regularity due to
  invalid reads/writes in routines starting with dl_*.  Why?

  - reductions/bradc/manual/promote
  - performance/sungeun/dgemm
  - studies/sudoku/dinan/sudoku


o types/string/StringImpl/memLeaks/* (gasnet*, gasnet.fifo): kyleb, hilde?

  https://chapel.atlassian.net/browse/CHAPEL-8

  These tests get sporadic failures of various flavors under gasnet.
  Sung tried to quiet them down at some point, but apparently didn't
  succeed.  Are the problems these tests are hitting related to other
  known AMM issues?  Will the tests have value when new strings come
  on-line?  Do Kyle's string tests have any similar noisiness on the
  string development branch?


o sporadic valgrind timeouts (valgrind): anyone

  https://chapel.atlassian.net/browse/CHAPEL-9

  These tests regularly cause noise in valgrind testing.  What can be
  done to improve that?  Are they just long-running?

  - domains/sungeun/assoc/stress.numthr
  - performance/bharshbarg/arr-forall
  - studies/shootout/fannkuch-redux/sungeun/fannkuch-redux


o sporadic invalid reads/writes (valgrind): anyone

  https://chapel.atlassian.net/browse/CHAPEL-10

  These tests started sporadically getting invalid reads/writes on
  4/11.  Did something change on 4/10 to cause this? (see also
  'invalid reads/writes' below)

  - exercises/boundedBuffer2
  - trivial/bradc/formatoutput


o pivotal story 91235766 (gasnet-fast): anyone

  https://chapel.atlassian.net/browse/CHAPEL-12

  This test is effectively in a "keep it in REGRESSIONS until someone
  determines that it shouldn't be" limbo which doesn't seem very
  useful.  What can we do to remove it?

  - multilocale/local/diten/test_local2


o bulkcomm execution timeouts (gasnet.fifo): anyone

  https://chapel.atlassian.net/browse/CHAPEL-13

  These tests seem to time out very regularly.  What can be done to
  stop that?  Do the tests themselves run too long, or are they
  hitting some sort of deadlock on occasion?

  - optimizations/bulkcomm/alberto/Block/2dBDtoDRTest
  - optimizations/bulkcomm/alberto/Block/2dDRtoBDTest


o execflags/bradc/gdbddash/gdbSetConfig (xc-wb.*): anyone

  https://chapel.atlassian.net/browse/CHAPEL-14

  Why oh why does this test time out so frequently on the xc-wb testing?


o sporadic x? HW execution timeouts (xe.ugni*): anyone

  https://chapel.atlassian.net/browse/CHAPEL-15

  Is this a fairly recently seen failure?  Any occurrences before 3/22?

  - release/examples/benchmarks/hpcc/variants/ra-cleanloop


o lulesh timeouts (xe.mpi.pgi, perf.xc.local.cray): anyone/bradc

  https://chapel.atlassian.net/browse/CHAPEL-16

  What would it take to make this stop timing out?


o sporadic [chpl_launchcmd] "output file from job does not exist..." errors (xc.*): anyone

  https://chapel.atlassian.net/browse/CHAPEL-17

  We seem to have been seeing this in the 03/25 onward timeframe, if not before

  - a bunch of tests... see REGRESSIONS


o sporadic slurm 'expired credential' problem (xc.*): anyone

  https://chapel.atlassian.net/browse/CHAPEL-18

  We seem to have gotten a few of these 4/13 and 4/16.  What's up?

  - see REGRESSIONS


o sporadic pgi failures (perf.xc.local.pgi): anyone

  https://chapel.atlassian.net/browse/CHAPEL-19

  Why are these failing regularly on PGI?  Note possible relation to
  'These started timing out consistently on 2/18' CHAPEL-20

  - studies/shootout/nbody/bradc/nbody-blc-slice
  - release/examples/benchmarks/hpcc/ptrans


o cygwin noise (cygwin*): 

  https://chapel.atlassian.net/browse/CHAPEL-21

  These tests bounce between error matching output and timing out.  Help?

  - studies/cholesky/jglewis/version2/performance/test_cholesky_performance
  - reductions/diten/testSerialReductions


o Colorado state tests noisy (or failing) on Cygwin (cygwin32): ben

  https://chapel.atlassian.net/browse/CHAPEL-22

  Some of these are noisy, others simply fail

  - studies/colostate/OMP-Jacobi-1D-Naive-Parallel
  - studies/colostate/OMP-Jacobi-2D-Naive-Parallel
  - studies/colostate/OMP-Jacobi-1D-Sliced-Diamond-Tiling


high priority (failing in one or more configurations)
-----------------------------------------------------

o regexp issues (valgrind, linux32, cygwin*, cygwin64): mppf/anyone

  https://chapel.atlassian.net/browse/CHAPEL-23

  - regexp/ferguson/rechan
    io/tzakian/recordReader/test

      These two Have gotten valgrind errors since enabled

  - regexp/ferguson/ctests/regexp_channel_test (linux32, cygwin*, cygwin64): mppf

      Fails in various ways on various platforms.  Not sure what to
      make of the fact that it's listed in cygwin* and cygwin64 with
      different failure modes.  Did someone mistakenly refactor a
      cygwin32 failure mode upwards?

  - regexp/ferguson/ctests/regexp_test

      Times out on linux32


high priority (open too long)
-----------------------------

o studies/glob/test_glob (baseline): bradc

  https://chapel.atlassian.net/browse/CHAPEL-24

  Failing since ???.  The claim was that the AMM improvements would
  fix this test, but who wants to wait for that?  This test exercises
  prototype routines that have since been matured and put into
  FileSystem.  Can the test be revamped to use the official routines?


o io/vass/time-write (perf.xc.no-local.gnu): vass

  https://chapel.atlassian.net/browse/CHAPEL-25

  Failing since first run?  Why?  Race in using printf()?

  - io/vass/time-write


o multilocale/deitz/needMultiLocales/test_remote_file_read (gasnet-fast): mppf

  https://chapel.atlassian.net/browse/CHAPEL-26

  This test has been failing with a segfault since 2/22.


o memory/bradc/allocBig (linux32): kyleb

  https://chapel.atlassian.net/browse/CHAPEL-27

  Chapel string passed to extern routine.  Didn't get fixed with the
  rest of the fixes that went in with/after that patch?  Has been
  failing since 3/5.


o modules/standard/BitOps/rotl (xc-wb.prgenv-cray): kyleb

  https://chapel.atlassian.net/browse/CHAPEL-28

  Error matching program output since 2015-03-10.


o fftw.h not found since  3/19 (xc.llvm): thomas

  https://chapel.atlassian.net/browse/CHAPEL-29

  - release/examples/primers/FFTWlib


o new errors on 2015-03-28 (valgrind): vass

  https://chapel.atlassian.net/browse/CHAPEL-30

  What happened on that night to make these tests start failing and
  continue to fail ever since?

  - performance/sungeun/assign_across_locales
  - studies/amr/diffusion/grid/Grid_DiffusionBE_driver
  - studies/amr/diffusion/level/Level_DiffusionBE_driver
  - studies/lulesh/bradc/xyztuple/lulesh-dense-3tuple


o chpldoc missing package (cygwin32): thomas

  https://chapel.atlassian.net/browse/CHAPEL-31

  And has been since 4/7.

  - chpldoc*


o ipe/powerOfTwo (valgrind, *32): noakes

  https://chapel.atlassian.net/browse/CHAPEL-32

  This test has been failing with a segfault on 32-bit systems since
  4/19; valgrind will probably point out the cause.


high priority (worrisome)
-------------------------

o invalid reads/writes (valgrind): anyone

  https://chapel.atlassian.net/browse/CHAPEL-11

  These are marked as being not manually reproducible, but they've
  failed so regularly since 4/11, how can that be?  Did something
  change on 4/10 to cause this?  (See also 'sporadic invalid
  reads/writes' above).

  - arrays/bradc/reindex/reindex]
  - domains/bradc/constdomain
  - domains/bradc/subdomain
  - spec/marybeth/for
  - spec/marybeth/select
  - users/ferguson/forall_expr


high priority (potential portability issues)
--------------------------------------------

o I/O test assertions fail (intel): mppf

  https://chapel.atlassian.net/browse/CHAPEL-33

  This test fails on intel which seems worrisome.

  - io/ferguson/writef_readf


o binary files differ (intel): mppf

  https://chapel.atlassian.net/browse/CHAPEL-34

  This test reports that its binary files differs on intel which
  seems worrisome.

  - io/ferguson/writefbinary


o factorization failed (intel): anyone

  https://chapel.atlassian.net/browse/CHAPEL-35

  This test seems to have started failing on 03/12 -- why?  How did we
  miss it?


high priority (CCE quiettude)
-----------------------------

o compilation timeouts (xe*cray): anyone

  https://chapel.atlassian.net/browse/CHAPEL-36

  The Cray compiler is notoriously slow.  What can be done?
  - release/examples/benchmarks/lulesh/lulesh
  - release/examples/benchmarks/miniMD/miniMD
  - release/examples/benchmarks/ssca2/SSCA2_main


o text file busy with path mandelbrot (xc-wb.prgenv-cray): thomas

  https://chapel.atlassian.net/browse/CHAPEL-37

  - exercises/Mandelbrot/mandelbrot


o invalid option "openmp" on the command line (xc-wb.prgenv-cray): ben/thomas

  https://chapel.atlassian.net/browse/CHAPEL-38

  - studies/colostate/OMP-Jacobi-1D-Naive-Parallel
  - studies/colostate/OMP-Jacobi-1D-Sliced-Diamond-Tiling
  - studies/colostate/OMP-Jacobi-2D-Naive-Parallel
  - studies/colostate/OMP-Jacobi-2D-Sliced-Diamond-Tiling


o types/enum/ferguson/enum_mintype_test (prgenv-cray): anyone

  https://chapel.atlassian.net/browse/CHAPEL-39

  This test gets a "value outside int rage" at C compilation time.
  What should we do about this?


o error differs but within acceptable margin (xc-wb.prgenv-cray): anyone

  https://chapel.atlassian.net/browse/CHAPEL-40

  - studies/hpcc/FFT/marybeth/fft


o consistent execution timeout (xc-wb.prgenv-cray): anyone

  https://chapel.atlassian.net/browse/CHAPEL-41

  What changed on 2015-02-15 to make this start timing out?  Why only
  here?

  - performance/compiler/bradc/cg-sparse-timecomp


o when building 'chpl' with CCE, weird stuff happens (xc-wb.host.prgenv-cray): anyone

  https://chapel.atlassian.net/browse/CHAPEL-42

  Proposed approach was to build compiler with CCE and run with
  valgrind to see if we're doing something bad under CCE.  bradc has
  notes but never had time to follow through.


o studies/hpcc/FFT/marybeth/fft (prgenv-cray): anyone

  https://chapel.atlassian.net/browse/CHAPEL-43

  Error differs, but within an acceptable margin; should squash
  printing of the error for correctness testing?


medium priority (worrisome)
---------------------------

o conditional jump in re2 (valgrind): anyone

  https://chapel.atlassian.net/browse/CHAPEL-44

  The following tests get a "conditional jump depends on uninitialized
  value" -- should we patch re2 to fix something?

  - io/tzakian/recordReader/test
  - regexp/ferguson/rechan


medium priority (clutter)
-------------------------

o memory/sungeun/refCount/domainMaps (*32): anyone

  https://chapel.atlassian.net/browse/CHAPEL-45

  Reports a different amount of memory leaked on 32-bit platforms.  Is
  this because something of pointer size is being leaked?  Can we
  quiet this somehow without losing whatever value it has?


o users/vass/type-tests.isSubType (*32): anyone/vass

  https://chapel.atlassian.net/browse/CHAPEL-46

  Consistently segfaulting since first run, according to REGRESSIONS.


o domains/sungeun/assoc/parSafeMember (pgi): anyone (ben?)

  https://chapel.atlassian.net/browse/CHAPEL-47

  This test has a tendency to time out under PGI.  Note that PGI
  doesn't support native atomics, so that may be the cause.  Could the
  test be reduced to get it under the timeout and rely instead on
  running across multiple nights for stability, or does it need to run
  this long?  Can the # iterations be set based on
  CHPL_TARGET_COMPILER in the sources?


o negative floating point 0.0 issue (pgi): anyone

  https://chapel.atlassian.net/browse/CHAPEL-48

  Something about -0.0i doesn't make PGI happy

  - types/complex/bradc/negateimaginary3
  - types/complex/bradc/negativeimaginaryliteral
  - types/file/bradc/scalar/floatcomplexexceptions


o QIO assertion errors (cygwin): mppf

  https://chapel.atlassian.net/browse/CHAPEL-49

  The following test gets QIO assertion errors.  Should it?

  - io/ferguson/ctests/qio_formatted_test (compopts: 1)


o -0.0 not supported on PGI? (pgi): anyone

  https://chapel.atlassian.net/browse/CHAPEL-50

  Are they within their rights with C or is this a portability issue or ...?
  Should we suppress?  Is there a flag/fix?

  - types/complex/bradc/negateimaginary3
  - types/complex/bradc/negativeimaginaryliteral
  - types/file/bradc/scalar/floatcomplexexceptions


o unclear how KNCs interact with the file system (xc.knc): anyone

  https://chapel.atlassian.net/browse/CHAPEL-51

  - release/examples/primers/fileIO


o OOB array access for non 2**k core count (perf.xc.*): ben

  https://chapel.atlassian.net/browse/CHAPEL-52

  I remember strategizing about this one at the perf meeting, but not the
  conclusion.  Also times out when it's not failing...

  - npb/is/mcahir/intsort.mtml


o These started timing out consistently on 2/18 (perf.xc.no-local.gnu): anyone

  https://chapel.atlassian.net/browse/CHAPEL-20

  Why would that be?

  - release/examples/benchmarks/hpcc/ptrans
  - studies/shootout/nbody/bradc/nbody-blc-slice
  - studies/shootout/nbody/sidelnik/nbody_forloop_3
  - studies/ssca2/memory/SSCA2_mem (see also 'SSCA2_main times out sporadically'?)


o "got == expect[i]" assertion error (cygwin32): mppf

  https://chapel.atlassian.net/browse/CHAPEL-53

  - io/ferguson/ctests/qio_bits_test


o consistent execution timeout / assertion error (cygwin32, cygwin64): mppf

  https://chapel.atlassian.net/browse/CHAPEL-54

  - io/ferguson/ctests/qio_test


medium priority (noisy)
-----------------------

o sporadic execution timeouts (gasnet.fifo): anyone

  https://chapel.atlassian.net/browse/CHAPEL-55

  This test has timed out a few times recently -- why is that?  What
  should be done with it?

  - studies/madness/aniruddha/madchap/test_mul


o sadSerial times out sporadically (perf.xc.local.intel): lydia

  https://chapel.atlassian.net/browse/CHAPEL-56

  - studies/parboil/SAD/sadSerial



low priority (annoying, but should get better in the future)
------------------------------------------------------------

o fasta-lines (perf.bradc-lnx, perf.chap03, perf.chapel-shootout): anyone

  https://chapel.atlassian.net/browse/CHAPEL-57

  This fails due to insane memory usage due to strings.  Can anything
  be done to improve this situation until strings get better?  Would
  using the new string_rec help?



low priority (portability issue, significant level of effort required)
----------------------------------------------------------------------

o release/examples/benchmarks/shootout/pidigits (llvm): anyone/thomas/mppf

  https://chapel.atlassian.net/browse/CHAPEL-58

  This fails due to the use of macros in gmp.h.  Thomas had a
  potential plan for dealing with it, which I forget offhnad.



low priority (so infrequent as to be potentially not our issue)
---------------------------------------------------------------

o SIGBUS at execution time (xc.knc): anyone

  https://chapel.atlassian.net/browse/CHAPEL-59

  I think we've seen this just once?

  - release/examples/benchmarks/lulesh/lulesh


o timed out once? (perf.xc.*): anyone

  https://chapel.atlassian.net/browse/CHAPEL-60

  Was this a one-off?

  - functions/iterators/angeles/distAdaptativeWSv2


o timed out once (perf.xc.local.cray): anyone

  https://chapel.atlassian.net/browse/CHAPEL-61

  Was this a one-off?

  - studies/shootout/fasta/kbrady/fasta-printf
