\sekshun{Memory Consistency Model}
\label{Memory_Consistency_Model}
\index{memory consistency model}

In this section, we describe Chapel's memory consistency model. The
model is based on \emph{sequential consistency for data-race-free}
programs as adopted by C11, C++11, Java, UPC, and Fortran 2008.

Sequential consistency (SC) means that all Chapel tasks agree on the
interleaving of memory operations and this interleaving results in an
order is consistent with the order of operations in the program source
code.  \emph{Conflicting memory operations}, i.e., operations to the
same variable, or memory location, and one of which is a write, form a data race if they
are from different Chapel tasks and can be executed concurrently.
Accesses to the same variable from different tasks can result from
the tasks referencing the same variable directly -- or indirectly
via aliases. Aliases arise, for example, when using \chpl{ref} variables,
argument intents, return intents, task intents and forall intents.
% TODO: add to the above: 'const ref' variables/intents and
% 'ref'/'const ref' fields, once we settle on their semantics.

Any Chapel program with a data race is not a valid program, and an
implementation cannot be relied upon to produce consistent behavior.
Valid Chapel programs will use synchronization constructs such
as \emph{sync}, \emph{single}, or \emph{atomic} variables or
higher-level constructs based on these to enforce ordering for
conflicting memory operations.

The following design principles were used in developing Chapel's memory
consistency model:

\begin{enumerate}

  \item Sequential programs have program order semantics.  Programs that
  are completely sequential cannot have data races and should appear to
  execute as though each statement was executed one at a time and in the
  expected order.

  \item Chapel's fork-join constructs introduce additional order
  dependencies.  Operations within a task cannot behave as though they
  started before the task started. Similarly, all operations in a task
  must appear to be completed to a parent task when the parent task joins
  with that task.

  \item Multi-locale programs have the same memory consistency model as
  single-locale programs. The Chapel language seeks to allow a single
  description of an algorithm to work with different data distributions.
  A result of this property is that an expression of a program must be
  correct whether it is working on local or distributed data.

  \item Chapel's memory model should be as relaxed as possible while still
  consistent with these design principles. In particular, making all
  operations sequentially consistent is not likely to enable good
  performance. At the same time, sequential consistency should be
  available to programmers when requested.

\end{enumerate}

See \textit{A Primer on Memory Consistency and Cache Coherence} by
Sorin, \emph{et~al.} for more background information on memory
consistency models. This chapter will proceed in a manner inspired by
the $XC$ memory model described there.

%% sungeun: Commented out from the original
%%
%% The memory consistency model for Chapel is similar to the C11 and C++11
%% memory consistency models, with some minor exceptions which are described
%% in section \ref{relating_to_C_MCM}.

%% Section \ref{SC_for_DRF} directly describes sequential consistency for
%% data-race-free programs as an alternative way of understanding the
%% recommended subset of the C11 and C++11 memory consistency
%% models.

%% This description does not fully specify the behavior of atomics using
%% non-SC ordering. While non-SC atomic operations are available in
%% Chapel, they are intended for advanced users only.  Advanced users
%% should refer to section \ref{relating_to_C_MCM} or
%% section \ref{relaxed_atomics}. Section \ref{relating_to_C_MCM}
%% describes how to use the C11 and C++11 specifications to understand
%% the behavior of Chapel programs using non-SC atomic
%% operations. Section
%% \ref{relaxed_atomics} extends the memory model described below to include
%% relaxed atomic operations.



\section{Sequential Consistency for Data-Race-Free Programs}
\label{SC_for_DRF}
\index{memory consistency model!sequential consistency for data-race-free programs}

Sequential consistency for data-race-free programs is described in
terms of two orders: \textit{program order} and \textit{memory
order}. The \textit{program order} $<_p$ is a partial order describing
serial or fork-join parallelism dependencies between variable reads
and writes. The \textit{memory order} $<_m$ is a total order that
describes the semantics of synchronizing memory operations
(via \chpl{atomic}, \chpl{sync} or \chpl{single} variables) with
sequential consistency. Non-SC atomic operations (described in
Section~\ref{non_sc_atomics}) do not create this total order.

Note that \chpl{sync/single} variables have memory consistency
behavior equivalent to a sequence of SC operations on \chpl{atomic}
variables.  Thus for the remainder of the chapter, we will primarily
focus on operations on \chpl{atomic} variables.

We will use the following notation:
\begin{itemize}

  \item $L(a)$ indicates a \textit{load} from a variable at address $a$.
  $a$ could refer to local or remote memory.

  \item $S(a)$ indicates a \textit{store} to a variable at address $a$.
  $a$ could refer to local or remote memory.

  \item $A_{sc}(a)$ indicates an \textit{atomic operation} on a variable
  at address $a$ with sequential consistency. The variable at address
  $a$ could refer to local or remote memory.  Atomic operations must
  be completed as a single operation (i.e. atomically), and so it is
  not possible to observe an intermediate state from an atomic
  operation under any circumstances.

  \item $A_r(a,o)$ indicates an \textit{atomic operation} on a
  variable at address $a$ with ordering constraint $o$, where $o$ can
  be one of \emph{relaxed}, \emph{acquire}, or \emph{release} (see
  Section~\ref{non_sc_atomics}).  As with $A_{sc}(a)$, relaxed atomic
  operations must be completed as a single operation.

  \item $L(a)$, $S(a)$, $A_{sc}(a)$, and $A_r(a,o)$ are also called
  \textit{memory operations}

  \item $X <_p Y$ indicates that $X$ precedes $Y$ in program order

  \item $X <_m Y$ indicates that $X$ precedes $Y$ in memory order

  \item \chpl{t = begin\{X\}} starts a new task named $t$ to execute $X$

  \item \chpl{waitFor($t_1$..$t_n$)} waits for tasks $t_1..t_n$ to complete

  \item \chpl{on(L)} migrates the running task to locale $L$. Note
that while the \chpl{on} statement may change the locale on which the
current task is running, it has no impact on the memory consistency
requirements.

\end{itemize}

For the purposes of describing this memory model, it is assumed that Chapel
programs will be translated into sequences of \textit{memory operations},
\chpl{begin} statements, and \chpl{waitFor} statements. The translation of a
Chapel program into a sequence of \textit{memory operations} must
preserve sequential program semantics. That is, if we have a snippet
of a Chapel program without task operations, such as \chpl{X; Y;}, the
statements $X$ and $Y$ will be converted into a sequence
of \textit{load}, \textit{store}, and \textit{atomic operations} in a
manner that preserves the behavior of a this serial portion of the
program. That is, $X=x_1,x_2,...$ and $Y=y_1,y_2,...$ where $x_i$ and
$y_j$ are each a sequence of \textit{load}, \textit{store},
or \textit{atomic operations} and we have $x_i <_p y_j$.

Likewise, for the purposes of this memory model, Chapel's parallelism
keywords are viewed as a sequence of operations including the
primitives of starting a task (\chpl{begin}) and waiting for some
number of tasks (\chpl{waitFor($t_1$..$t_n$)}). In particular:

\begin{itemize}

  \item \chpl{forall} (including promotion) creates some number of
tasks $m$ to execute the $n$ iterations of the loop

(\chpl{$t_i$ =
begin\{some-loop-bodies\}} for each task $i=1$..$m$) and waits for
them to complete (\chpl{waitFor($t_1$..$t_m$)}).  The number of tasks $m$
is defined by the implementation of the parallel iterator (See
Section~\ref{Iterators} for details on iterators).
%% sungeun: I don't think we need to define a forall here, i.e., say
%% that every iteration must be independent
%%
%% Brad suggested dropping forall all together.  What do you think?
%%
%% michael: I think it's worth describing for the clarity, even
%% if it's not strictly necessary.

  \item \chpl{coforall} creates one task per loop iteration
(\chpl{$t_i$ = begin\{loop-body\}} for all loop iterations $i=1..n$)
and then waits for them all to complete (\chpl{waitFor($t_1$..$t_n$)}).

  \item \chpl{cobegin} creates one task per enclosed statement
(\chpl{$t_i$ = begin\{$X_i$\}} for statements $X_1$..$X_n$) and then
waits for them all to complete (\chpl{waitFor($t_1$..$t_n$)}).

  \item \chpl{begin} creates a task to execute the enclosed statement
(\chpl{t = begin\{X\}}).  The \chpl{sync} statement waits for all
tasks $t_i$ created by a \chpl{begin} statement in the dynamic scope of the
\chpl{sync} statement that are not within other, nested \chpl{sync}
statements (\chpl{waitFor($t_1$..$t_n$)} for all $n$ such tasks).

\end{itemize}

\subsection{Program Order}
\label{program_order}

Task creation and task waiting create a conceptual tree of program
statements.  The task bodies, task creation, and task wait operations
create a partial order $<_p$ of program statements.  For the purposes
of this section, the statements in the body of each Chapel task will
be implemented in terms of \textit{load}, \textit{store}, and
\textit{atomic operations}.

\begin{itemize}
  \item If we have a program snippet without tasks, such as \chpl{X; Y;},
  where $X$ and $Y$ are memory operations, then $X <_p Y$.

  \item The program \chpl{X; begin\{Y\}; Z;} implies $X$ $<_p$ $Y$.
  However, there is no particular relationship between $Y$ and $Z$ in
  program order.

  \item The program \chpl{t = begin\{Y\}; waitFor(t); Z;} implies $Y$ $<_p$ $Z$

  \item $X$ $<_p$ $Y$ and $Y$ $<_p$ $Z$ imply $X$ $<_p$ $Z$

\end{itemize}


\subsection{Memory Order}
\label{memory_order}

The memory order $<_m$ of SC atomic operations in a given task
respects program order as follows:

\begin{itemize}
  \item If $A_{sc}(a)<_pA_{sc}(b)$ then $A_{sc}(a)<_mA_{sc}(b)$
\end{itemize}

Every SC atomic operation gets its value from the last SC atomic
operation before it to the same address in the total order $<_m$:
%% sungeun: Brad didn't like max, but I couldn't think of anything better
\begin{itemize}
  \item Value of $A_{sc}(a)$ = Value of $max_{<_m} (
  A_{sc}'(a)|A_{sc}'(a) <_m A_{sc}(a) ) $
\end{itemize}

For data-race-free programs, every load gets its value from the last
store before it to the same address in the total order $<_m$:
\begin{itemize}
  \item Value of $L(a)$ = Value of $max_{<_m}$ $( S(a)|S(a)$ $<_m$ $L(a)$ or $S(a)$ $<_p$ $L(a) )$
\end{itemize}

For data-race-free programs, loads and stores are ordered with SC
atomics.  That is, loads and stores for a given task are in total
order $<_m$ respecting the following rules which preserve the order of
loads and stores relative to SC atomic operations:

%Note that if a program contains data
%races, the \textit{memory order} is not an order at all for the racy loads
%$L(a)$ and stores $S(a)$ because these loads and stores do not have to be
%completed at once (in other words, two racing stores to the same address could
%result in a value being written that is neither of the written values but is a
%combination of the two writes).

\begin{itemize}
  \item If $L(a)<_pA_{sc}(b)$ then $L(a)<_mA_{sc}(b)$
  \item If $S(a)<_pA_{sc}(b)$ then $S(a)<_mA_{sc}(b)$
  \item If $A_{sc}(a)<_pL(b)$ then $A_{sc}(a)<_mL(b)$
  \item If $A_{sc}(a)<_pS(b)$ then $A_{sc}(a)<_mS(b)$
\end{itemize}

For data-race-free programs, loads and stores preserve sequential
program behavior.  That is, loads and stores to the same address in a
given task are in the order $<_m$ respecting the following rules which
preserve sequential program behavior:

\begin{itemize}
  \item If $L(a) <_p L'(a)$ then $L(a) <_m L'(a)$
  \item If $L(a) <_p S(a)$ then $L(a) <_m S(a)$
  \item If $S(a) <_p S'(a)$ then $S(a) <_m S'(a)$
\end{itemize}


% This table is commented out because I'm not totally sure it's correct.
% Or at least it might be misleading. In particular, I don't feel that
% this conveys that transformations are allowed only if they wouldn't
% change sequential program behavior. In particular, if an address
% is computed based on the result of one load, the second load can't go
% before the first load.
%
% B = load A
% load B
%
% these loads are to different addresses but the order must be preserved.
%
% The table below describes which operations can be reordered with other
% operations in the translation from program order to the global order.  The rows
% indicate the first operation and the columns indicate the second operation. An
% X indicates that the ordering is always enforced. An A indicates that the
% ordering is only enforced if the operations are to the same address.
% 
% \begin{center}
% \begin{tabular}{|r|r|r|r|}
% \hline
%        & Load & Store & Atomic \\ \hline
% Load   &    A &     A &      X \\ \hline
% Store  &    A &     A &      X \\ \hline
% Atomic &    X &     X &      X \\ \hline
% \end{tabular}
% \end{center}




% The following is commented out because it doesn't quite fit here.

% I don't think this belongs in the MCM section. I think it's valuable, but
% inappropriate for it to be here in it's current form. I'm guessing this is
% meant to lead into some of the discussions we had around optimizing data
% structures and allowing them to be invalid until a "flush", but it's current
% form talks about implementation specific aspects of our data structures.
%\section{Data Structure Consistency}
%
%Generally speaking, data structures provided by the Chapel language and
%libraries are not safe for concurrent use when one of those uses is
%modification. In particular,
%
%\begin{itemize}
%
%\item Elements of a Chapel array are not safe against \textit{data races}
%unless they are of an \textit{atomic} or \textit{sync} type.
%
%\item Chapel arrays and domains are not safe for concurrent changes to
%their shape.
%
%\item Chapel arrays are safe for concurrent updates to different elements
%that already exist or for read-only access from multiple tasks.
%
%\end{itemize}
%
%Data structure authors should make an effort to document how their data
%structure is safe or is not safe for concurrent updates. Generally speaking,
%data structures might fall into these categories:
%
%\begin{itemize}
%
%\item safe for any concurrent updates (e.g. a work queue)
%
%\item safe for concurrent updates to disjoint regions but not for shape
%changes (e.g. an array)
%
%\item not safe for any concurrent updates, even to disjoint regions (e.g.
%the a string that reallocates itself when a character is changed)
%
%\end{itemize}
%
%The data structure consistency could operate with either of these
%strategies:
%
%\begin{itemize}
%
%\item data structure updates will be visible once the usual task
%synchronization takes place, or
%
%\item the data structure requires consistency to be managed explicitly
%(e.g. with a 'flush' call).
%
%\end{itemize}






% The following is commented out because it's long and probably not
% worthwhile in the grand scheme of things. As it is now, programs using
% acquire/release are relying on implementation details.
% We could bring it back in the future if necessary.

%% \section{Relationship to the C11 and C++11 memory models}
%% \label{relating_to_C_MCM}
%% \index{memory consistency model!relationship to C memory consistency model}

%% We have already shown an outline of the memory consistency model for
%% Chapel.  Some features of the language allow you to request an ordering
%% more relaxed than SC ordering. This section describes how to understand
%% Chapel programs in terms of the C11 and C++11 specifications. We will
%% summarize some of the features of the C11 and C++11 memory models. These
%% summaries should be taken only as an aid in understanding. Although we
%% refer to the C++11 model specifically, the C11 memory model is extremely
%% similar.

%% \subsection{Understanding the C++11 memory model}

%% The C++11 specification describes the memory consistency model in terms of
%% a total order on atomic operations \textit{separately for each atomic
%% memory location}. It does \textit{not} guarantee a global total order
%% on atomic operations unless sequential consistency is always used. In
%% particular, for relaxed consistency atomics, the operations on the atomics
%% do complete in a total order \textit{for each atomic memory location}.  In
%% other words, at any given moment, an atomic at address $a$ updated with a
%% relaxed atomic operation by any number of threads will either have the
%% result of the update or not. It cannot have an intermediate state.
%% However, since it is a total order for each address, relaxed atomic
%% operations to two different memory locations could be reordered.

%% The C++11 memory models also uses some key terms which we will summarize here:

%% \begin{itemize}

%%   \item \textit{thread} an execution context for a stream of instructions.

%%   \item \textit{acquire semantics} means that no memory operation can move
%%   from after the acquire operation to before the acquire operation.  Only
%%   loads or atomic operations that read from memory can have
%%   \textit{acquire semantics}.  For example, a load after an acquire
%%   operation cannot return a value cached from before the acquire
%%   operation.

%%   \item \textit{release semantics} means that no memory operation can move
%%   from before the release operation to after the release operation.  Only
%%   stores or atomic operations that write to memory can have
%%   \textit{release semantics}.  For example, a store started before a
%%   release operation must complete before the release operation completes.

%%   \item \textit{synchronizes with} expresses how threaded programs need
%%   memory operations to be ordered. A \textit{release} operation to a particular
%%   memory location M \textit{synchronizes with} an \textit{acquire} operation
%%   that takes its value from the \textit{release} operation. In addition, thread
%%   creation operation synchronizes with the new thread; and the thread
%%   completion synchronizes with the joining operation in the parent thread.

%%   \item \textit{sequenced before} is a relationship indicating which operations
%%   must always occur before other operations within the same thread in order to
%%   preserve sequential program semantics.

%%   \item \textit{happens before} is a relationship indicating which operations
%%   must always occur before other operations according to the memory consistency
%%   model. \textit{happens before} includes the ordering required by
%%   \textit{synchronizes with} as well as the ordering required by
%%   \textit{sequenced before}.

%% \end{itemize}
 
%% The C++11 specifications provides the following memory orders that can be used to request particular memory consistency
%% for each atomic operation. These memory orders are also available in
%% Chapel.
%% % C standard section 5.1.2.4 p 20, 7.17.3 and 7.17.4 p 277. C++ standard section 1.10 p11 and and 29 p 1113
%% \begin{itemize}
%%   \item \chpl{memory\_order\_relaxed} For an atomic operation, the value of the
%%   atomic variable itself will be updated in a single operation, but the order
%%   of other loads and stores or the order of relaxed atomic operations to other
%%   addresses is not necessarily preserved.

%%   \item \chpl{memory\_order\_acquire} a load or read-modify-write operation has \textit{acquire
%%   semantics}.

%%   \item \chpl{memory\_order\_release} a store or read-modify-write operation has \textit{release
%%   semantics}.

%%   \item \chpl{memory\_order\_acq\_rel} a store operation has \textit{release
%%   semantics} and a load operation has \textit{acquire semantics}.

%%   \item \chpl{memory\_order\_seq\_cst} Includes the properties of both
%%   \chpl{memory\_order\_acquire} and \chpl{memory\_order\_release} and
%%   additionally requires that there be a total ordering on
%%   memory\_order\_seq\_cst operations. This is the default and corresponds
%%   to SC ordering in section \ref{SC_for_DRF} above.

%% \end{itemize}

%% \subsection{Mapping Chapel concepts to the C++11 memory model}
%% sungeun: this subsection moved to the next section edited but
%% comment out for use elsewhere
%%

%% To understand the semantics of programs using non-SC atomics, it is
%% necessary to refer to the C11/C++11 memory model. Chapel programs will
%% follow the C11/C++11 memory model with the following additional :
%% \begin{itemize}
%%   \item a \textit{task} in Chapel corresponds to a \textit{thread} in C++
%%   for the purposes of the memory model.

%%   \item Chapel's parallel language features map to thread creation and
%%   thread joining in the C++11 memory model. In particular, the
%%   language features are described above in terms of \chpl{begin}
%%   and \chpl{waitFor}. \chpl{begin} corresponds to thread creation
%%   and \chpl{waitFor} corresponds to thread joining.

%% \begin{openissue}
%%   We do not intend to prevent optimizations that preserve a Chapel
%%   program's behavior simply because the Chapel program uses tasks.
%%   For example, it should be possible to optimize the following program
%%   to one that sets A = 1 and B = 2 without ever creating a task. The
%%   above description might prevent such optimization.

%% \begin{chapel}
%% var A = 1;
%% var B = 0;
%% cobegin with (ref B) {
%%   B = 2;
%% }
%% \end{chapel}

%% \end{openissue}

%%   \item Chapel's \chpl{sync} variables are equivalent to locks in the
%%   C++11 memory model.

%%   \item The idea of \textit{program order} described above replaces the
%%   C++11 notion of \textit{sequenced before}.

%%   \item Whether data is local or remote has no effect on memory consistency.

%% \end{itemize}


\section{Non-Sequentially Consistent Atomic Operations}
\label{non_sc_atomics}
\index{memory consistency model!non-sequentially consisten atomic operations}

Sequential consistency for atomic operations can be a performance
bottleneck under some circumstances.  Chapel provides non-SC atomic
operations to help alleviate such situations.  Such uses of atomic
operations must be done with care and should generally not be used to
synchronize tasks.

Non-SC atomic operations are specified by providing a \emph{memory
order} argument to the atomic operations.  See
Section~\ref{Functions_on_Atomic_Variables} for more information on
the memory order types.

\subsection{Relaxed Atomic Operations}
\label{relaxed_atomics}
%% sungeun: As per Brad's suggestion let's move this text to a ``best
%% practices'' document or something like that.
%%
%% The least error prone ways to use \chpl{memory\_order\_relaxed} is for
%% variables that need atomic updates but that are not used to synchronize
%% tasks. An example would be a running total computed by multiple tasks.

%% It is just as unreasonable to synchronize tasks with a non-atomic variable
%% as it is to synchronize them with a \chpl{memory\_order\_relaxed} atomic
%% variable. Both of these are disastrous for program correctness and should
%% be avoided. Note that advanced users can combine
%% \chpl{memory\_order\_relaxed} with fences to synchronize tasks.

Although Chapel's relaxed atomic operations
(\chpl{memory\_order\_relaxed}) do not complete in a total order by
themselves and might contribute to non-deterministic programs, relaxed
atomic operations cannot contribute to a data race that would prevent
sequential consistency.

When relaxed atomics are used only for atomicity and not as part of
synchronizing tasks, their effect can be understood in the memory
consistency model described in \ref{SC_for_DRF} by treating them as
ordinary loads or stores with two exceptions:

\begin{itemize}

\item Atomic operations (including relaxed atomic operations) cannot
create data races.

\item All atomic operations (including relaxed atomic operations) will
eventually be visible to all other threads. This property is not true for
normal loads and stores.
%% sungeun: for our own reference
%% This property corresponds to paragraph 25 in the
%% C++ specification section 1.10.

\end{itemize}

\section{Unordered Memory Operations}
\label{unordered_operations}
\index{memory consistency model!unordered memory operations}

\begin{openissue}
Syntax for \textit{unordered} operations has not yet been finalized.
\end{openissue}

\begin{openissue}
Should Chapel provide a memory fence that only completes unordered
operations started by the current task?
\end{openissue}
\begin{openissue}
Should unordered operations on a particular memory address always wait
for the address to be computed?
\end{openissue}
\begin{openissue}
Does starting a task or joining with a task necessarily wait for
unordered operations to complete?
%% sungeun: commenting out for our own notes.
%%
%% (It would appear so from the current
%% description and the C++11 semantics where thread creation and joining
%% create a \textit{synchronizes with} relationship).
%% (but the current version does not explicitly depend on the C++11
%% semantics, so this is not an issue)
\end{openissue}

Rather than issuing normal loads and stores to read or write local or
remote memory, a Chapel program can use \textit{unordered} loads and
stores when preserving sequential program behavior is not
important.  The following notation for unordered memory operations
will be used in this section:

\begin{itemize}

  \item $UL(a)$ indicates an \textit{unordered} \textit{load} from a
  variable at address $a$. $a$ could point to local or remote memory.

  \item $US(a)$ indicates an \textit{unordered} \textit{store} to a
  variable at address $a$. Again, $a$ could point to local or remote
  memory.

\end{itemize}

The \textit{unordered} loads and stores $UL(a)$ and $US(a)$ respect
fences but not program order. As in Section~\ref{memory_order},
unordered loads and stores are ordered with SC atomics.  That is,
unordered loads and stores for a given task are in total order $<_m$
respecting the following rules which preserve the order of unordered
loads and stores relative to SC atomic operations:

\begin{itemize}
  \item If $UL(a)<_pA_{sc}(b)$ then $UL(a)<_mA_{sc}(b)$
  \item If $US(a)<_pA_{sc}(b)$ then $US(a)<_mA_{sc}(b)$
  \item If $A_{sc}(a)<_pUL(b)$ then $A_{sc}(a)<_mUL(b)$
  \item If $A_{sc}(a)<_pUS(b)$ then $A_{sc}(a)<_mUS(b)$
\end{itemize}

Unordered loads and stores do not preserve sequential program behavior.

\subsection{Unordered Memory Operations Examples}

Unordered operations should be thought of as happening in a way that
overlaps with the program task. Unordered operations started in
different program statements can happen in any order unless an SC
atomic operation orders them.

Since unordered operations started by a single task can happen in any
order, totally sequential programs can have a data race when using
unordered operations. This follows from our original definition of
data race.

\begin{chapel}
var x: int = 0;
unordered_store(x, 10);
unordered_store(x, 20);
writeln(x);
\end{chapel}

The value of \textit{x} at the end of this program could be 0, 10, or 20. As a
result, programs using unordered loads and stores are not sequentially
consistent unless the program can guarantee that unordered operations
can never operate on the same memory at the same time when one of them is a
store. In particular, the following are safe:

\begin{itemize}
  \item Unordered stores to disjoint regions of memory.
  \item Unordered loads from potentially overlapping regions of memory when no store could overlap with the loads.
  \item Unordered loads and stores to the same memory location
  when these are always separated by an SC atomic operation.
\end{itemize}

Unordered loads and stores are available as a performance
optimization. For example, a program computing a permutation on an array might
want to move data between two arrays without requiring any ordering:

\begin{chapel}
const n = 10;
// P is a permutation on 1..n, in this case reversing its input
var P = for i in 1..n by -1 do i;
// A is an array to permute
var A = for i in 1..n do i;
// Compute, in B, the permutation applied to A
var B:[1..n] int;

for i in 1..n {
  unordered_store(B[P[i]], A[i]);
}
\end{chapel}


%%
%% sungeun: Commented out for now.
%% michael: the reasoning here is worth understanding and recording somewhere.
%%          comments are fine..
%% \section{Discussion}

%% Chapel's memory model does not include the \textit{write atomicity} or
%% \textit{store atomicity} property for general variable writes for two reasons.
%% First, an RDMA message could be partway through copying some data when another
%% thread reads that data. Second, two portions of the machine could have some
%% local hardware caches that complete writes for other cores at different times
%% from further away cores as allowed by the C11 or C++11 memory models. Sorin,
%% Hill and Wood define \textit{write atomicity} as the property that a store
%% operation is logically seen by all other cores at once (section 5.5.2). This
%% property is upheld for SC operations on \chpl{atomic} variables by
%% construction (since these are in a total order). However, \textit{write
%% atomicity} is not upheld for all atomic operations. For a given moment in any
%% task's execution, a write to any \chpl{atomic} variable write is either
%% entirely completed or entirely not yet started. This property is guaranteed for
%% any \chpl{atomic} variables with any ordering constraint including relaxed.
%% However, the property that a given write is seen by all other tasks at once is
%% not upheld for atomic operations unless the SC ordering is used.

%% For a distributed memory system, the straightforward implementation of SC
%% atomics or sync variables will be sequentially consistent. The straightforward
%% implementation is to follow these rules:

%% \begin{itemize}

%%  \item every task issues these SC atomic operations in program order

%%  \item each SC atomic operation is performed at the home locale of the
%%  atomic variable in question (e.g. with an Active Message or with network
%%  hardware support)

%%  \item each task does not start another operation until the SC atomic operation
%%  it is working on has completed.

%% \end{itemize}

%% In that case, the reasoning in Adve Hill "Implementing Sequential Consistency
%% in Cache-Based Systems" applies and the executions of these synchronization
%% operations will be sequentially consistent.

%% If the SC atomic operations themselves are sequentially consistent and the
%% program is free of data races, the load and store operations will also be
%% sequentially consistent. The reasoning here is that:

%% \begin{itemize}

%%  \item The load and store operations cannot be reordered across SC atomic
%%  operations (see \ref{loads_stores_ordered_with_atomics}).

%%  \item Concurrent access by at least two tasks to the same memory location,
%%  when at least one of the accesses is a store, is a race condition. In order to
%%  avoid race conditions and preserve SC semantics, accesses to the same memory
%%  location from different tasks when one of the accesses is a store must be
%%  mediated by SC atomic operations.

%%  \item The SC atomic operation constrains the order of the load and store
%%  operations so that they appear as a total order.
%% \end{itemize}

%% Sketch of proof. Suppose that a data-race-free program produces a non-SC
%% outcome. That would mean that there is no total ordering on the loads or stores
%% to a particular memory location. If the memory location is never updated in the
%% relevant program region, the loads could be considered to be in any order in
%% the total order and provide the same result. If at least one of the operations
%% is a store, and it is not already constrained by the SC atomic operations, then
%% there is a race condition.

%(Corollary 1: LLVM optimizations and C program optimizations are OK because C programs cannot reorder SC atomic ops)
%(Corollary 2: Cache is OK because:
% - any communication not mediated by atomics constitutes a data race
% - use of SC atomics causes cache flush/invalidate)

%Adve phd thesis
%Scheurich and Dubois ScD87 and Sch89 - sufficient condition for sequential consistency.
%Collier proved .... writes in same order equivalent to atomically... thus system ... is sequential consistenc. Col84-92 and DuS90.
%LHH91 also do it. All writes are seen in the same order by all processors -> sequentially consistent.
%\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.217.8176&rep=rep1&type=pdf}: An execution is consistent with respect to a consistency model... and these 3 invariants are sufficient to guarantee memory consistency defined in this way: Uniprocessor Ordering, Allowable Reordering, and Cache Coherence.
%i1. typical implementation is coherent:
% coherent "a coherent system must appear to execute all thread's loads and stores to a single memory location in a total order that respects the program order of each thread".
% Sufficient Conditions for SC:
%  - Every processor issues memory ops in program order
%  - Processor must wait for store to complete before issuing next memory operation
%  - After load, issuing proc waits for load to complete, and store that produced value to complete before issuing next op
%Scheurich Dubois and Briggs 1988 Synchronization, Coherence, and Event Ordering in Multiprocessors.
%\url{ftp://ftp.cs.wisc.edu/markhill/Papers/icpp90_seqcon.pdf} sufficient conditions for SC including the above.
%\url{http://www.cs.berkeley.edu/~kubitron/cs252/lectures/lec20-sharedmemory3.pdf} has a picture of "exclusion zone" and argument for why processor atomics -> sequential consistency.

\section{Examples}
\label{MCM_examples}
\index{memory consistency model!examples}
%\begin{chapelexample}{syncFenceFlag}
\begin{example}
  In this example, a synchronization variable is used to (a) ensure that
  all writes to an array of unsynchronized variables are complete, (b)
  signal that fact to a second task, and (c) pass along the number of
  values that are valid for reading.

  The program
\begin{chapel}
var A: [1..100] real;
var done(*\texttt{\$}*): sync int;           // initially empty
cobegin {
  { // Reader task
    const numToRead = done(*\texttt{\$}*);   // block until writes are complete
    for i in 1..numToRead do
      writeln("A[", i, "] = ", A[i]);
  }
  {  // Writer task
    const numToWrite = 14;     // an arbitrary number
    for i in 1..numToWrite do
      A[i] = i/10.0;
    done(*\texttt{\$}*) = numToWrite;        // fence writes to A and signal done
  }
}
\end{chapel}
  produces the output
\begin{chapelprintoutput}{}
A[1] = 0.1
A[2] = 0.2
A[3] = 0.3
A[4] = 0.4
A[5] = 0.5
A[6] = 0.6
A[7] = 0.7
A[8] = 0.8
A[9] = 0.9
A[10] = 1.0
A[11] = 1.1
A[12] = 1.2
A[13] = 1.3
A[14] = 1.4
\end{chapelprintoutput}
%\end{chapelexample}
\end{example}


\begin{chapelexample}{syncSpinWait.chpl}
One consequence of Chapel's memory consistency model is that a task cannot spin-wait on a
normal variable waiting for another task to write to that variable.  The behavior of
the following code is undefined:

\begin{chapelpre}
if false { // }
\end{chapelpre}
\begin{chapel}
var x: int;
cobegin with (ref x) {
  while x != 1 do ;  // INCORRECT spin wait
  x = 1;
}
\end{chapel}
\begin{chapelnoprint}
// {
}
\end{chapelnoprint}
In contrast, spinning on a synchronization variable has well-defined
behavior:
\begin{chapel}
var x(*\texttt{\$}*): sync int;
cobegin {
  while x(*\texttt{\$}*).readXX() != 1 do ;  // spin wait
  x(*\texttt{\$}*).writeXF(1);
}
\end{chapel}
\begin{chapeloutput}
\end{chapeloutput}

In this code, the first statement in the cobegin statement executes a
loop until the variable is set to one.  The second statement in the
cobegin statement sets the variable to one.  Neither of these
statements block.
\end{chapelexample}

\begin{chapelexample}{atomicSpinWait.chpl}
Atomic variables provide an alternative means to spin-wait.
For example:

\begin{chapel}
var x: atomic int;
cobegin {
  while x.read() != 1 do ;  // spin wait - monopolizes processor
  x.write(1);
}
\end{chapel}
\begin{chapeloutput}
\end{chapeloutput}

\end{chapelexample}


\begin{chapelexample}{atomicWaitFor.chpl}
The main drawback of the above example is that it prevents the thread
executing the spin wait from doing other useful work. Atomic variables
include a waitFor method that will block the calling thread until a read
of the atomic value matches a particular value. In contrast to the spin
wait loop above, waitFor will allow other tasks to be scheduled. For
example:

\begin{chapel}
var x: atomic int;
cobegin {
  x.waitFor(1);
  x.write(1);
}
\end{chapel}
\begin{chapeloutput}
\end{chapeloutput}

\end{chapelexample}


\begin{future}
Upon completion, Chapel's atomic statement~(\rsec{Atomic_Statement}) will serve as
an additional means of correctly synchronizing between tasks.
\end{future}

